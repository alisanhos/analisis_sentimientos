{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae511335",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia Artificial\n",
    "\n",
    "## An치lisis de sentimientos\n",
    "\n",
    "Realizado por:\n",
    "- Alicia S치nchez Hossdorf\n",
    "- Rafael Segura G칩mez\n",
    "\n",
    "Fecha: 26/05/2023\n",
    "\n",
    "Convocatoria de Junio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2343c24",
   "metadata": {},
   "source": [
    "<h1> 1. Recopilaci칩n de datos. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0958c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba71c79",
   "metadata": {},
   "source": [
    "Hemos descargado todos los tweets que necesitamos en un CSV. Por lo que debemos leer los datos del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bc99f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento.csv\", header=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874309a0",
   "metadata": {},
   "source": [
    "Para saber cuantos tweets tenemos, miraremos cuantas filas tiene el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "82b203ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweetsCSV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7b3ada5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweetsCSV.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eee3ec",
   "metadata": {},
   "source": [
    "Como solo necesitamos la columna text, que es donde viene los tweets que necesitamos, haremos una seleccion de dicha columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3d919204",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweetsCSV[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "670f9a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test from the LG enV2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S`ok, trying to plot alternatives as we speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i`ve been sick for the past few days  and thus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hes just not that into you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0                 I`d have responded, if I were going\n",
       "1       Sooo SAD I will miss you here in San Diego!!!\n",
       "2                           my boss is bullying me...\n",
       "3                      what interview! leave me alone\n",
       "4    Sons of ****, why couldn`t they put them on t...\n",
       "5   http://www.dothebouncy.com/smf - some shameles...\n",
       "6   2am feedings for the baby are fun when he is a...\n",
       "7                                          Soooo high\n",
       "8                                         Both of you\n",
       "9    Journey!? Wow... u just became cooler.  hehe....\n",
       "10   as much as i love to be hopeful, i reckon the...\n",
       "11  I really really like the song Love Story by Ta...\n",
       "12       My Sharpie is running DANGERously low on ink\n",
       "13  i want to go to music tonight but i lost my vo...\n",
       "14                         test test from the LG enV2\n",
       "15                              Uh oh, I am sunburned\n",
       "16   S`ok, trying to plot alternatives as we speak...\n",
       "17  i`ve been sick for the past few days  and thus...\n",
       "18         is back home now      gonna miss every one\n",
       "19                         Hes just not that into you"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd330f5a",
   "metadata": {},
   "source": [
    "<h1> 2. Eliminar las palabras que no aportan informaci칩n </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e7cc2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e6bcf0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "003ce35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('punkt', download_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b52f1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"shouldn't\", 'nor', 've', 'an', 'here', 'o', \"don't\", 'myself', 'll', 'who', 'hers', 'his', 'more', 'herself', \"it's\", \"aren't\", 're', 'me', 'these', 's', 'now', 'over', 'be', 'once', \"weren't\", 'all', 'ma', 'any', 'too', 'couldn', 'had', \"hasn't\", 'of', 'ain', 'only', 'yours', 'is', 'yourself', \"haven't\", 'why', 'are', 'that', \"should've\", 'or', \"you'll\", 'your', 'own', 'with', \"you'd\", 'further', 'how', 'not', 'i', \"isn't\", 'having', 'ours', 'we', 'can', 'they', 'ourselves', 'and', 'them', \"that'll\", 'haven', \"mightn't\", 'have', 't', 'before', 'yourselves', 'which', 'for', 'm', 'been', 'don', 'should', 'through', 'than', 'such', 'this', 'most', \"she's\", 'did', 'out', 'again', 'between', 'wasn', 'shouldn', 'off', 'then', 'but', 'because', 'each', 'up', 'very', 'in', 'her', 'while', 'isn', 'until', 'some', 'no', 'below', 'to', 'he', 'himself', 'other', 'weren', 'where', 'so', \"didn't\", 'am', 'there', \"won't\", 'were', 'theirs', \"couldn't\", \"needn't\", 'mightn', 'few', 'it', 'at', 'just', \"hadn't\", \"wouldn't\", 'd', 'our', 'shan', 'hasn', 'needn', 'down', 'aren', 'she', 'when', \"mustn't\", 'didn', 'about', 'their', 'if', 'does', 'those', 'as', 'doing', 'into', 'doesn', 'same', 'has', 'what', \"shan't\", 'against', 'will', 'won', 'during', 'themselves', 'do', 'was', 'the', \"wasn't\", \"doesn't\", \"you've\", 'from', 'on', 'him', 'a', 'wouldn', 'you', 'my', 'its', 'after', 'under', 'y', 'mustn', \"you're\", 'both', 'by', 'above', 'whom', 'being', 'itself', 'hadn'}\n"
     ]
    }
   ],
   "source": [
    "sWords = set(stopwords.words('english'))\n",
    "print(sWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698cfa1",
   "metadata": {},
   "source": [
    "Crear una funci칩n clean_text para eliminar las palabras comunes y poco informativas; eliminar menciones, hashtags, URLs y cualquier otro s칤mbolo extra침o; y que utilice t칠cnicas de lematizaci칩n (stemming) y tokenizaci칩n para reducir las palabras a su forma base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "43976e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar las stop words y tokenizado.\n",
    "\n",
    "def delete_stopWords(tweet):\n",
    "    tweetLow = tweet.lower()\n",
    "    tweetWords = word_tokenize(tweetMin)\n",
    "    tweetFiltered = []\n",
    "        \n",
    "    for w in tweetWords:\n",
    "        if w not in sWords:\n",
    "            tweetFiltered.append(w)\n",
    "    return tweetFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "301d7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion es para eliminar menciones, hashtags, URLs y cualquier otro s칤mbolo extra침o.\n",
    "\n",
    "def delete_otherElements(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"http[^\\s]+\",\"\",tweet)\n",
    "    tweetFiltered = re.sub(r\"[-()\\\"#/@;:<>{}+=*~|.!?,]\", \"\", tweet)\n",
    "\n",
    "    return tweetFiltered\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb9f7",
   "metadata": {},
   "source": [
    "## Funci칩n para limpiar el text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe91be",
   "metadata": {},
   "source": [
    "Con los tweets tokenizados, lematizados y sin stop words lo pasamos a lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7ea662b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(Tweets): \n",
    "    res = Tweets.copy()\n",
    "    i = 0;\n",
    "    while i<len(tweets):\n",
    "        tweet_cleaned = []\n",
    "        clean_t = delete_otherElements(Tweets['text'].get(i))\n",
    "        tNoStopWords = delete_stopWords(clean_t)\n",
    "        #Esto es para la lematizaci칩n de la frase. \n",
    "        for w in tNoStopWords:\n",
    "            if w != \"`\":\n",
    "                tweet_cleaned.append(PorterStemmer().stem(w))\n",
    "        res.at[i,'text'] = tweet_cleaned\n",
    "        \n",
    "        i+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "75694d7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweetMin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tweets_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m tweets_cleaned\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[122], line 7\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(Tweets)\u001b[0m\n\u001b[0;32m      5\u001b[0m tweet_cleaned \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m clean_t \u001b[38;5;241m=\u001b[39m delete_otherElements(Tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(i))\n\u001b[1;32m----> 7\u001b[0m tNoStopWords \u001b[38;5;241m=\u001b[39m \u001b[43mdelete_stopWords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Esto es para la lematizaci칩n de la frase. \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tNoStopWords:\n",
      "Cell \u001b[1;32mIn[120], line 5\u001b[0m, in \u001b[0;36mdelete_stopWords\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelete_stopWords\u001b[39m(tweet):\n\u001b[0;32m      4\u001b[0m     tweetLow \u001b[38;5;241m=\u001b[39m tweet\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m----> 5\u001b[0m     tweetWords \u001b[38;5;241m=\u001b[39m word_tokenize(\u001b[43mtweetMin\u001b[49m)\n\u001b[0;32m      6\u001b[0m     tweetFiltered \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tweetWords:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweetMin' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_cleaned = clean_text(tweets)\n",
    "tweets_cleaned.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1b3d",
   "metadata": {},
   "source": [
    "## tagdo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00e10b",
   "metadata": {},
   "source": [
    "Documentar:\n",
    "- !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acaa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_text(text):\n",
    "    tweet = ' '.join(text)  # Convertir la lista de palabras en una cadena de text\n",
    "    blob = TextBlob(tweet)\n",
    "    tag = blob.sentiment.polarity #Entre -1 y 1, de peor a mejor respectivamente\n",
    "    return tweet, tag\n",
    "\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "\n",
    "with open(csv_path, 'w', newline='',encoding='utf-8-sig') as csv_file: #Forzamos formato utf-8, otro da problemas\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['text', 'tag'])\n",
    "    \n",
    "    if len(tweets_cleaned) == 0:\n",
    "        print(\"칂Nothing to tag, espabila notas.\")\n",
    "        exit()\n",
    "\n",
    "    for _, row in tweets_cleaned.iterrows(): #Usamos iterrows porque tweets_cleaned es de tipo dataframe\n",
    "        resultado = tag_text(row['text'])\n",
    "        writer.writerow(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98810428",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_results = pandas.read_csv(\"../Notebook/Tweets/resultados.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fc0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea702d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 primeras filas\n",
    "tweets_results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33210926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
