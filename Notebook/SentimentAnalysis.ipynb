{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae511335",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia Artificial\n",
    "\n",
    "## Análisis de sentimientos\n",
    "\n",
    "Realizado por:\n",
    "- Alicia Sánchez Hossdorf\n",
    "- Rafael Segura Gómez\n",
    "\n",
    "Fecha: 26/05/2023\n",
    "\n",
    "Convocatoria de Junio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2343c24",
   "metadata": {},
   "source": [
    "<h1> 1. Recopilación de datos. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0958c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba71c79",
   "metadata": {},
   "source": [
    "Hemos descargado todos los tweets que necesitamos en un CSV. Por lo que debemos leer los datos del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc99f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento.csv\", header=0)\n",
    "tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento_small.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874309a0",
   "metadata": {},
   "source": [
    "Para saber cuantos tweets tenemos, miraremos cuantas filas tiene el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b203ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweetsCSV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3ada5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                I`d have responded, if I were going   neutral  \n",
       "1                                           Sooo SAD  negative  \n",
       "2                                        bullying me  negative  \n",
       "3                                     leave me alone  negative  \n",
       "4                                      Sons of ****,  negative  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                fun  positive  \n",
       "7                                         Soooo high   neutral  \n",
       "8                                        Both of you   neutral  \n",
       "9                       Wow... u just became cooler.  positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweetsCSV.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eee3ec",
   "metadata": {},
   "source": [
    "Como solo necesitamos la columna text, que es donde viene los tweets que necesitamos, haremos una seleccion de dicha columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d919204",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweetsCSV[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670f9a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test from the LG enV2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S`ok, trying to plot alternatives as we speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i`ve been sick for the past few days  and thus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hes just not that into you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0                 I`d have responded, if I were going\n",
       "1       Sooo SAD I will miss you here in San Diego!!!\n",
       "2                           my boss is bullying me...\n",
       "3                      what interview! leave me alone\n",
       "4    Sons of ****, why couldn`t they put them on t...\n",
       "5   http://www.dothebouncy.com/smf - some shameles...\n",
       "6   2am feedings for the baby are fun when he is a...\n",
       "7                                          Soooo high\n",
       "8                                         Both of you\n",
       "9    Journey!? Wow... u just became cooler.  hehe....\n",
       "10   as much as i love to be hopeful, i reckon the...\n",
       "11  I really really like the song Love Story by Ta...\n",
       "12       My Sharpie is running DANGERously low on ink\n",
       "13  i want to go to music tonight but i lost my vo...\n",
       "14                         test test from the LG enV2\n",
       "15                              Uh oh, I am sunburned\n",
       "16   S`ok, trying to plot alternatives as we speak...\n",
       "17  i`ve been sick for the past few days  and thus...\n",
       "18         is back home now      gonna miss every one\n",
       "19                         Hes just not that into you"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd330f5a",
   "metadata": {},
   "source": [
    "<h1> 2. Eliminar las palabras que no aportan información </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7cc2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6bcf0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003ce35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('punkt', download_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b52f1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'having', 'any', 'been', 'through', 'we', 'or', 'there', 'ourselves', 'hasn', 'as', 'nor', 'each', 'are', 'again', \"she's\", 'wouldn', 'only', \"wouldn't\", 'did', 'to', 'isn', 'be', 'by', 'into', \"wasn't\", 'those', 'this', 'i', 'ours', 'my', 've', \"isn't\", \"it's\", 'on', 'out', 'mightn', \"mightn't\", \"weren't\", \"hasn't\", 'below', 'myself', 'very', 'because', \"won't\", 'do', 'so', 'after', 'during', 'of', 'here', 'should', 'himself', 'your', 'off', 'against', 'between', 'up', 'same', 'most', 'theirs', 'her', 'were', 'then', 'doing', \"needn't\", \"you're\", 'its', 'has', 'ma', 'in', 'll', 'she', 'above', \"doesn't\", 'but', 'am', 'weren', 'further', 'few', 'just', 'if', 'that', 'you', 'which', 'where', \"don't\", 'wasn', 'and', 'our', 'who', \"shan't\", 'whom', 'about', \"aren't\", 'the', \"hadn't\", 'ain', 'what', 'all', 'needn', 'at', 'o', 'until', 'haven', 'once', 'both', 'such', 'm', 'yourselves', \"that'll\", 'yourself', 'while', 'own', 'themselves', 'aren', \"didn't\", 'down', 'over', 'too', 'a', 't', 'y', 'me', 'will', 'yours', 'it', 'than', 'didn', 'itself', 'was', 'from', 'for', 'no', 'he', 'have', 'can', 'other', 'does', \"shouldn't\", 'with', 'how', 'not', 'shouldn', 'under', 'shan', 'hadn', \"couldn't\", 'being', 'an', 'mustn', 'him', \"you'll\", 'why', 'their', 'is', 's', 'hers', 'doesn', 'herself', 'had', 'more', 'them', 'they', 'some', \"mustn't\", 'don', 'his', 'couldn', \"you'd\", 'now', 'when', 're', \"you've\", \"should've\", 'won', 'these', 'before', 'd', \"haven't\"}\n"
     ]
    }
   ],
   "source": [
    "sWords = set(stopwords.words('english'))\n",
    "print(sWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698cfa1",
   "metadata": {},
   "source": [
    "Crear una función clean_text para eliminar las palabras comunes y poco informativas; eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño; y que utilice técnicas de lematización (stemming) y tokenización para reducir las palabras a su forma base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13942f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para corregir gramaticalmente\n",
    "\n",
    "def correctGrammar_tweets(tweet):\n",
    "    tweetBlob = TextBlob(tweet)\n",
    "    return tweetBlob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9fb35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar las stop words y tokenizado.\n",
    "\n",
    "def delete_stopWords(tweet):\n",
    "    tweetLow = tweet.lower()\n",
    "    tweetWords = word_tokenize(tweetLow)\n",
    "    tweetFiltered = []\n",
    "        \n",
    "    for w in tweetWords:\n",
    "        if w not in sWords:\n",
    "            tweetFiltered.append(w)\n",
    "    return tweetFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "301d7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion es para eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño.\n",
    "\n",
    "def delete_otherElements(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"http[^\\s]+\",\"\",tweet)\n",
    "    tweetFiltered = re.sub(r\"[-()\\\"#/@;:<>{}+=*~|.!?,]\", \"\", tweet)\n",
    "\n",
    "    return tweetFiltered\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb9f7",
   "metadata": {},
   "source": [
    "## Función para limpiar el text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe91be",
   "metadata": {},
   "source": [
    "Con los tweets tokenizados, lematizados y sin stop words lo pasamos a lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ea662b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(Tweets): \n",
    "    res = Tweets.copy()\n",
    "    i = 0;\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    while i<len(tweets):\n",
    "        tweet_cleaned = []\n",
    "        \n",
    "        clean_t = delete_otherElements(Tweets['text'].get(i))\n",
    "        tNoStopWords = delete_stopWords(clean_t)\n",
    "        #Esto es para la lematización de la frase. \n",
    "        for w in tNoStopWords:\n",
    "            if w != \"`\":\n",
    "                tweet_cleaned.append(stemmer.stem(w))\n",
    "        res.at[i,'text'] = tweet_cleaned\n",
    "        \n",
    "        i+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75694d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned = clean_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6ef69c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text\n",
      "0                                       [respond, go]\n",
      "1                       [sooo, sad, miss, san, diego]\n",
      "2                                       [boss, bulli]\n",
      "3                             [interview, leav, alon]\n",
      "4                 [son, put, releas, alreadi, bought]\n",
      "5       [shameless, plug, best, ranger, forum, earth]\n",
      "6                  [2am, feed, babi, fun, smile, coo]\n",
      "7                                       [soooo, high]\n",
      "8                                                  []\n",
      "9     [journey, wow, u, becam, cooler, hehe, possibl]\n",
      "10  [much, love, hope, reckon, chanc, minim, p, ne...\n",
      "11  [realli, realli, like, song, love, stori, tayl...\n",
      "12                    [sharpi, run, danger, low, ink]\n",
      "13             [want, go, music, tonight, lost, voic]\n",
      "14                             [test, test, lg, env2]\n",
      "15                                  [uh, oh, sunburn]\n",
      "16               [ok, tri, plot, altern, speak, sigh]\n",
      "17  [sick, past, day, thus, hair, look, wierd, did...\n",
      "18            [back, home, gon, na, miss, everi, one]\n",
      "19                                              [hes]\n"
     ]
    }
   ],
   "source": [
    "print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26a16439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  \\\n",
      "0                                       [respond, go]   \n",
      "1                       [sooo, sad, miss, san, diego]   \n",
      "2                                       [boss, bulli]   \n",
      "3                             [interview, leav, alon]   \n",
      "4                 [son, put, releas, alreadi, bought]   \n",
      "5       [shameless, plug, best, ranger, forum, earth]   \n",
      "6                  [2am, feed, babi, fun, smile, coo]   \n",
      "7                                       [soooo, high]   \n",
      "8                                                  []   \n",
      "9     [journey, wow, u, becam, cooler, hehe, possibl]   \n",
      "10  [much, love, hope, reckon, chanc, minim, p, ne...   \n",
      "11  [realli, realli, like, song, love, stori, tayl...   \n",
      "12                    [sharpi, run, danger, low, ink]   \n",
      "13             [want, go, music, tonight, lost, voic]   \n",
      "14                             [test, test, lg, env2]   \n",
      "15                                  [uh, oh, sunburn]   \n",
      "16               [ok, tri, plot, altern, speak, sigh]   \n",
      "17  [sick, past, day, thu, hair, look, wierd, didn...   \n",
      "18            [back, home, gon, na, miss, everi, one]   \n",
      "19                                               [he]   \n",
      "\n",
      "                                       text_corregido  \n",
      "0                                          respond go  \n",
      "1                             soon sad miss san diego  \n",
      "2                                           boss bull  \n",
      "3                               interview leave along  \n",
      "4                      son put release already bought  \n",
      "5              shameless plug best ranger forum earth  \n",
      "6                          am feed baby fun smile too  \n",
      "7                                           soon high  \n",
      "8                                                      \n",
      "9           journey now u became cooper here possible  \n",
      "10  much love hope reckon chance minims p never on...  \n",
      "11    really really like song love story taylor swift  \n",
      "12                           sharp run danger low ink  \n",
      "13                   want go music tonight lost voice  \n",
      "14                                  test test ll envy  \n",
      "15                                     up oh sunburnt  \n",
      "16                       ok try plot alter speak sigh  \n",
      "17  sick past day the hair look wired didn hat wou...  \n",
      "18                     back home on na miss every one  \n",
      "19                                                 he  \n"
     ]
    }
   ],
   "source": [
    "# Supongamos que tu dataframe se llama \"df\" y la columna con los textos se llama \"text\"\n",
    "tweets_cleaned['text_corregido'] = tweets_cleaned['text'].apply(lambda x: str(TextBlob(\" \".join(x)).correct()))\n",
    "\n",
    "# Muestra el dataframe con los textos corregidos\n",
    "print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4cdde9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_corregido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>respond go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soon sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boss bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview leave along</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>son put release already bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>'you ride one catch one summer til pop open one '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>good gorjuz yea no ask yesterday the hospital ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>ok pop say hi check thing probably head gutta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>now really mission family today added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>source say</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_corregido\n",
       "0                                          respond go\n",
       "1                             soon sad miss san diego\n",
       "2                                           boss bull\n",
       "3                               interview leave along\n",
       "4                      son put release already bought\n",
       "..                                                ...\n",
       "73  'you ride one catch one summer til pop open one '\n",
       "74  good gorjuz yea no ask yesterday the hospital ...\n",
       "75  ok pop say hi check thing probably head gutta ...\n",
       "76              now really mission family today added\n",
       "77                                         source say\n",
       "\n",
       "[78 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra el dataframe con los textos corregidos\n",
    "tweets_cleaned[['text_corregido']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1b3d",
   "metadata": {},
   "source": [
    "## Etiquetado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00e10b",
   "metadata": {},
   "source": [
    "Documentar:\n",
    "- !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1acaa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01e1847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_text(text):\n",
    "    tweet = ' '.join(text)  # Convertir la lista de palabras en una cadena de text\n",
    "    blob = TextBlob(tweet)\n",
    "    tag = blob.sentiment.polarity #Entre -1 y 1, de peor a mejor respectivamente\n",
    "    return tweet, tag\n",
    "\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "\n",
    "with open(csv_path, 'w', newline='',encoding='utf-8-sig') as csv_file: #Forzamos formato utf-8, otro da problemas\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['text', 'tag'])\n",
    "    \n",
    "    if len(tweets_cleaned) == 0:\n",
    "        print(\"ÇNothing to tag, espabila notas.\")\n",
    "        exit()\n",
    "\n",
    "    for _, row in tweets_cleaned.iterrows(): #Usamos iterrows porque tweets_cleaned es de tipo dataframe\n",
    "        result = tag_text(row['text'])\n",
    "        writer.writerow(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98810428",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_results = pandas.read_csv(\"../Notebook/Tweets/resultados.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d2fc0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 2)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ea702d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>respond go</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boss bulli</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview leav alon</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>son put releas alreadi bought</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shameless plug best ranger forum earth</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feed babi fun smile coo</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>soooo high</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>journey wow u becam cooler hehe possibl</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>much love hope reckon chanc minim p never gon ...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>realli realli like song love stori taylor swift</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sharpi run danger low ink</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>want go music tonight lost voic</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test lg env2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>uh oh sunburn</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ok tri plot altern speak sigh</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sick past day thu hair look wierd didnt hat wo...</td>\n",
       "      <td>-0.482143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>back home gon na miss everi one</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>he</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text       tag\n",
       "0                                          respond go  0.000000\n",
       "1                             sooo sad miss san diego -0.500000\n",
       "2                                          boss bulli  0.000000\n",
       "3                                 interview leav alon  0.000000\n",
       "4                       son put releas alreadi bought  0.000000\n",
       "5              shameless plug best ranger forum earth  1.000000\n",
       "6                         2am feed babi fun smile coo  0.300000\n",
       "7                                          soooo high  0.160000\n",
       "8                                                 NaN  0.000000\n",
       "9             journey wow u becam cooler hehe possibl  0.100000\n",
       "10  much love hope reckon chanc minim p never gon ...  0.500000\n",
       "11    realli realli like song love stori taylor swift  0.500000\n",
       "12                          sharpi run danger low ink  0.000000\n",
       "13                    want go music tonight lost voic  0.000000\n",
       "14                                  test test lg env2  0.000000\n",
       "15                                      uh oh sunburn  0.000000\n",
       "16                      ok tri plot altern speak sigh  0.500000\n",
       "17  sick past day thu hair look wierd didnt hat wo... -0.482143\n",
       "18                    back home gon na miss everi one  0.000000\n",
       "19                                                 he  0.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweets_results.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
