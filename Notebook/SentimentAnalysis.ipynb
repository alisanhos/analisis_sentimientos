{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae511335",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia Artificial\n",
    "\n",
    "## Análisis de sentimientos\n",
    "\n",
    "Realizado por:\n",
    "- Alicia Sánchez Hossdorf\n",
    "- Rafael Segura Gómez\n",
    "\n",
    "Fecha: 26/05/2023\n",
    "\n",
    "Convocatoria de Junio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2343c24",
   "metadata": {},
   "source": [
    "<h1> 1. Recopilación de datos. </h1>\n",
    "\n",
    "Importamos un par de bibliotecas que nos haran falta posteriormente, y comprobación de si tenemos ya instaladas las librerias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e65abc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium is already installed\n",
      "Textblob is already installed\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "if importlib.util.find_spec(\"selenium\") is None:\n",
    "    !pip install selenium\n",
    "    print(\"Installed selenium succesfully\")\n",
    "else:\n",
    "    print(\"Selenium is already installed\")\n",
    "    \n",
    "if importlib.util.find_spec(\"textblob\") is None:\n",
    "    !pip install textblob\n",
    "    print(\"Installed Textblob succesfully\")\n",
    "else:\n",
    "    print(\"Textblob is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0958c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba71c79",
   "metadata": {},
   "source": [
    "Hemos descargado todos los tweets que necesitamos en un CSV. Por lo que debemos leer los datos del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc99f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874309a0",
   "metadata": {},
   "source": [
    "Para saber cuantos tweets tenemos, miraremos cuantas filas tiene el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b203ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweetsCSV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3ada5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                I`d have responded, if I were going   neutral  \n",
       "1                                           Sooo SAD  negative  \n",
       "2                                        bullying me  negative  \n",
       "3                                     leave me alone  negative  \n",
       "4                                      Sons of ****,  negative  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                fun  positive  \n",
       "7                                         Soooo high   neutral  \n",
       "8                                        Both of you   neutral  \n",
       "9                       Wow... u just became cooler.  positive  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweetsCSV.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eee3ec",
   "metadata": {},
   "source": [
    "Como solo necesitamos la columna text, que es donde estan los tweets que necesitamos, haremos una seleccion de dicha columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d919204",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweetsCSV[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670f9a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test from the LG enV2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S`ok, trying to plot alternatives as we speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i`ve been sick for the past few days  and thus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hes just not that into you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0                 I`d have responded, if I were going\n",
       "1       Sooo SAD I will miss you here in San Diego!!!\n",
       "2                           my boss is bullying me...\n",
       "3                      what interview! leave me alone\n",
       "4    Sons of ****, why couldn`t they put them on t...\n",
       "5   http://www.dothebouncy.com/smf - some shameles...\n",
       "6   2am feedings for the baby are fun when he is a...\n",
       "7                                          Soooo high\n",
       "8                                         Both of you\n",
       "9    Journey!? Wow... u just became cooler.  hehe....\n",
       "10   as much as i love to be hopeful, i reckon the...\n",
       "11  I really really like the song Love Story by Ta...\n",
       "12       My Sharpie is running DANGERously low on ink\n",
       "13  i want to go to music tonight but i lost my vo...\n",
       "14                         test test from the LG enV2\n",
       "15                              Uh oh, I am sunburned\n",
       "16   S`ok, trying to plot alternatives as we speak...\n",
       "17  i`ve been sick for the past few days  and thus...\n",
       "18         is back home now      gonna miss every one\n",
       "19                         Hes just not that into you"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd330f5a",
   "metadata": {},
   "source": [
    "<h1> 2. Eliminar las palabras que no aportan información </h1>\n",
    "\n",
    "Importamos las bibliotecas de NLTK y Regex, para filtrar, tokenizar y, en general, tratar el texto para simplificarlo y poder trabajarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7cc2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bcf0bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "003ce35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('punkt', download_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52f1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'same', 'aren', \"needn't\", 'such', 'does', 'were', 'me', 'but', 'when', 'further', 'if', 'some', 'should', 'his', 'yours', 'we', 'did', 'on', 'all', 'ain', 'after', 'yourselves', 'her', 'myself', \"you're\", \"that'll\", 'needn', 'there', 'very', 'than', \"won't\", 'hasn', 'mustn', 'are', \"shan't\", 'no', 'i', 'wouldn', 'what', 'ourselves', \"didn't\", 'them', 'each', 'during', 'few', 'haven', 'in', \"aren't\", 's', \"isn't\", 'off', 'is', 'now', 'hers', 'how', 'these', \"wasn't\", 'as', 'for', 't', 'been', 'then', 'again', 'those', 'it', 'other', 'being', 'too', 'you', 'd', 'or', \"doesn't\", 'down', 'their', 'itself', 'its', 'between', 'll', 'have', 'doing', 'hadn', 'mightn', 'o', \"should've\", 'to', 'the', 'only', 'this', 'himself', 'has', 'didn', 'by', 'own', 'at', \"you'll\", 'themselves', \"haven't\", 'having', 'will', 'be', 'through', 'so', \"you've\", 'from', 'above', 'where', 'an', \"hadn't\", 'he', 're', 'not', 'doesn', 'ours', 'about', 'whom', 'theirs', 'against', 'a', 'yourself', 'over', 'ma', 'our', 'wasn', 'and', 'into', 'weren', 'with', 'don', 'she', \"mustn't\", 'any', \"shouldn't\", 'do', 'more', 'both', \"wouldn't\", 'just', 'won', 'am', \"it's\", \"mightn't\", 'here', 'had', 'him', 'under', 'they', 'who', 'can', 'most', 'm', \"couldn't\", \"she's\", 've', \"you'd\", \"hasn't\", 'nor', 'once', \"don't\", 'while', 'couldn', 'shan', 'isn', 'y', 'why', 'out', 'which', 'up', 'before', 'your', 'because', 'was', 'until', \"weren't\", 'herself', 'shouldn', 'that', 'my', 'below', 'of'}\n"
     ]
    }
   ],
   "source": [
    "sWords = set(stopwords.words('english'))\n",
    "print(sWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698cfa1",
   "metadata": {},
   "source": [
    "Crear una función clean_text para eliminar las palabras comunes y poco informativas; eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño; y que utilice técnicas de lematización (stemming) y tokenización para reducir las palabras a su forma base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83a7bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar las stop words y tokenizado.\n",
    "\n",
    "def delete_stopWords(tweet):\n",
    "    tweetLow = tweet.lower()\n",
    "    tweetWords = word_tokenize(tweetLow)\n",
    "    tweetFiltered = []\n",
    "        \n",
    "    for w in tweetWords:\n",
    "        if w not in sWords:\n",
    "            tweetFiltered.append(w)\n",
    "    return tweetFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "301d7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion es para eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño mediante regex.\n",
    "\n",
    "def delete_otherElements(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"http[^\\s]+\",\"\",tweet)\n",
    "    tweetFiltered = re.sub(r\"[-()\\\"#/@;:<>{}+=*~|.!?,]\", \"\", tweet)\n",
    "\n",
    "    return tweetFiltered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb9f7",
   "metadata": {},
   "source": [
    "## Función para limpiar el text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe91be",
   "metadata": {},
   "source": [
    "Con los tweets tokenizados, lematizados y sin stop words lo pasamos a lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ea662b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(Tweets): \n",
    "    res = Tweets.copy()\n",
    "    i = 0;\n",
    "    #Iniciar SnowBall Stemmer con idioma ingles\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    while i<len(Tweets):\n",
    "        tweet_cleaned = []\n",
    "        \n",
    "        clean_t = delete_otherElements(Tweets['text'].get(i))\n",
    "        \n",
    "        #__Para corregir gramaticalmente descomentar la siguiente linea__\n",
    "        #clean_t = str(TextBlob(clean_t).correct())\n",
    "        \n",
    "        tNoStopWords = delete_stopWords(clean_t)\n",
    "        #Esto es para la lematización de la frase. \n",
    "        for w in tNoStopWords:\n",
    "            if w != \"`\":\n",
    "                tweet_cleaned.append(stemmer.stem(w))\n",
    "        res.at[i,'text'] = tweet_cleaned\n",
    "        \n",
    "        i+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75694d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos los tweets aplicandole la función\n",
    "tweets_cleaned = clean_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6ef69c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text\n",
      "0                                       [respond, go]\n",
      "1                       [sooo, sad, miss, san, diego]\n",
      "2                                       [boss, bulli]\n",
      "3                             [interview, leav, alon]\n",
      "4                 [son, put, releas, alreadi, bought]\n",
      "5       [shameless, plug, best, ranger, forum, earth]\n",
      "6                  [2am, feed, babi, fun, smile, coo]\n",
      "7                                       [soooo, high]\n",
      "8                                                  []\n",
      "9     [journey, wow, u, becam, cooler, hehe, possibl]\n",
      "10  [much, love, hope, reckon, chanc, minim, p, ne...\n",
      "11  [realli, realli, like, song, love, stori, tayl...\n",
      "12                    [sharpi, run, danger, low, ink]\n",
      "13             [want, go, music, tonight, lost, voic]\n",
      "14                             [test, test, lg, env2]\n",
      "15                                  [uh, oh, sunburn]\n",
      "16               [ok, tri, plot, altern, speak, sigh]\n",
      "17  [sick, past, day, thus, hair, look, wierd, did...\n",
      "18            [back, home, gon, na, miss, everi, one]\n",
      "19                                              [hes]\n"
     ]
    }
   ],
   "source": [
    "print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa48272",
   "metadata": {},
   "source": [
    "Se implementó una correción gramatical de los tweets pero es inviable en un set de datos de semejante tamaño, lo comentamos para no usarlo ahora, pero lo dejamos por si hiciera falta la funcionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc5c7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrige el dataframe y muestra ambas columnas, corregido y original, no modifica variables originales\n",
    "#tweets_cleaned['text_corregido'] = tweets_cleaned['text'].apply(lambda x: str(TextBlob(\" \".join(x)).correct()))\n",
    "\n",
    "\n",
    "#print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1710b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra el dataframe con los textos corregidos\n",
    "#tweets_cleaned[['text_corregido']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1b3d",
   "metadata": {},
   "source": [
    "## Etiquetado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00e10b",
   "metadata": {},
   "source": [
    "Creamos la funcion para etiquetar texto y ver la polaridad, el sentimiento, entre -1 y 1 de peor a mejor, respectivamente. Escribimos en un archivo _resultado.csv_ los textos con la columna de la polaridad correspondiente, para trabajarla posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01e1847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text):\n",
    "    tweet = ' '.join(text)  # Convertir la lista de palabras en una cadena de text\n",
    "    blob = TextBlob(tweet) #Convierte los tweets a tipo Blob para aplicarle la libreria\n",
    "    tag = blob.sentiment.polarity #Polaridad entre -1 y 1, de peor a mejor respectivamente\n",
    "    return text, tag\n",
    "\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "\n",
    "#Escritura en un archivo CSV nuevo\n",
    "with open(csv_path, 'w', newline='',encoding='utf-8-sig') as csv_file: #Forzamos formato utf-8, otro da problemas\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['text', 'tag', 'sentiment']) #Crea columna sentiment también\n",
    "    \n",
    "    if len(tweets_cleaned) == 0:  #Checkea si está vacío\n",
    "        print(\"Nothing to tag, espabila notas.\")\n",
    "        exit()\n",
    "\n",
    "    for _, row in tweets_cleaned.iterrows(): #Usamos iterrows porque tweets_cleaned es de tipo dataframe\n",
    "       # if row['text']:  # Verificar si el campo 'text' no está vacío\n",
    "            result = tag_text(row['text'])\n",
    "            result = (*result, '')  # Añadir una cadena vacía para la columna \"sentiment\"\n",
    "            writer.writerow(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa76a0",
   "metadata": {},
   "source": [
    "Clasificamos los sentimientos entre -1 y 1. Se ha elegido 0 como neutro, de -1 a -0.5 para ser hater, de -0.5 a 0 para ser molesto y simétricamente para los positivos, de 0 a 0.5 para contento y de 0.5 a 1 para muy feliz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ed685d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorizando los tags a valores discretos\n",
    "def etiqueta_codificada(etiqueta):\n",
    "    if etiqueta >= -1 and etiqueta < -0.5:\n",
    "        return \"hater\"\n",
    "    elif etiqueta >= -0.5 and etiqueta < 0:\n",
    "        return \"molesto\"\n",
    "    elif etiqueta == 0:\n",
    "        return \"neutro\"\n",
    "    elif etiqueta > 0 and etiqueta <= 0.5:\n",
    "        return \"contento\"\n",
    "    elif etiqueta > 0.5 and etiqueta <= 1:\n",
    "        return \"muy feliz\"\n",
    "    else:\n",
    "        return \"null\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c248d02",
   "metadata": {},
   "source": [
    "Actualizamos el _resultados.csv_ con la columna _Sentiment_, el valor discreto relacionado al ordinal del _tag_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4829e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizar la columna sentiment del csv\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "df = pandas.read_csv(csv_path)\n",
    "\n",
    "# Aplicar la función etiqueta_codificada para asignar los valores de tag a sentiment\n",
    "df['sentiment'] = df['tag'].apply(etiqueta_codificada)\n",
    "\n",
    "# Guardar el archivo CSV actualizado\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98810428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la variable con la lectura del archivo resultado.csv\n",
    "tweets_results = pandas.read_csv(\"../Notebook/Tweets/resultados.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d2fc0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 3)\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos visualmente si está bien\n",
    "print(tweets_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ea702d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['respond', 'go']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['sooo', 'sad', 'miss', 'san', 'diego']</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['boss', 'bulli']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['interview', 'leav', 'alon']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['son', 'put', 'releas', 'alreadi', 'bought']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['shameless', 'plug', 'best', 'ranger', 'forum...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>muy feliz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['2am', 'feed', 'babi', 'fun', 'smile', 'coo']</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['soooo', 'high']</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['journey', 'wow', 'u', 'becam', 'cooler', 'he...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>['much', 'love', 'hope', 'reckon', 'chanc', 'm...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>['realli', 'realli', 'like', 'song', 'love', '...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>['sharpi', 'run', 'danger', 'low', 'ink']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>['want', 'go', 'music', 'tonight', 'lost', 'vo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>['test', 'test', 'lg', 'env2']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>['uh', 'oh', 'sunburn']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>['ok', 'tri', 'plot', 'altern', 'speak', 'sigh']</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>['sick', 'past', 'day', 'thus', 'hair', 'look'...</td>\n",
       "      <td>-0.482143</td>\n",
       "      <td>molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>['back', 'home', 'gon', 'na', 'miss', 'everi',...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>['hes']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text       tag  sentiment\n",
       "0                                   ['respond', 'go']  0.000000     neutro\n",
       "1             ['sooo', 'sad', 'miss', 'san', 'diego'] -0.500000    molesto\n",
       "2                                   ['boss', 'bulli']  0.000000     neutro\n",
       "3                       ['interview', 'leav', 'alon']  0.000000     neutro\n",
       "4       ['son', 'put', 'releas', 'alreadi', 'bought']  0.000000     neutro\n",
       "5   ['shameless', 'plug', 'best', 'ranger', 'forum...  1.000000  muy feliz\n",
       "6      ['2am', 'feed', 'babi', 'fun', 'smile', 'coo']  0.300000   contento\n",
       "7                                   ['soooo', 'high']  0.160000   contento\n",
       "8                                                  []  0.000000     neutro\n",
       "9   ['journey', 'wow', 'u', 'becam', 'cooler', 'he...  0.100000   contento\n",
       "10  ['much', 'love', 'hope', 'reckon', 'chanc', 'm...  0.500000   contento\n",
       "11  ['realli', 'realli', 'like', 'song', 'love', '...  0.500000   contento\n",
       "12          ['sharpi', 'run', 'danger', 'low', 'ink']  0.000000     neutro\n",
       "13  ['want', 'go', 'music', 'tonight', 'lost', 'vo...  0.000000     neutro\n",
       "14                     ['test', 'test', 'lg', 'env2']  0.000000     neutro\n",
       "15                            ['uh', 'oh', 'sunburn']  0.000000     neutro\n",
       "16   ['ok', 'tri', 'plot', 'altern', 'speak', 'sigh']  0.500000   contento\n",
       "17  ['sick', 'past', 'day', 'thus', 'hair', 'look'... -0.482143    molesto\n",
       "18  ['back', 'home', 'gon', 'na', 'miss', 'everi',...  0.000000     neutro\n",
       "19                                            ['hes']  0.000000     neutro"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweets_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b249095",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "Necesitamos separar los datos que tenemos en conjunto de entrenamiento y conjunto de pruebas, para probar que funciona bien e ir entrenando el modelo.\n",
    "\n",
    "Importamos todas las librerias necesarias para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4b26a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14288914",
   "metadata": {},
   "source": [
    "Sacamos la columna _tweets_ del CSV. Creamos el vectorizador vacío para codificar el texto en un vector disperso y poder trabajar facilmente con los datos, ya que son miles de palabras diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bd4b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cojo la columna text\n",
    "tweets = tweets_results['text']\n",
    "\n",
    "#Creamos el vector\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#Vectorizamos los tweets\n",
    "codified_text = vect.fit_transform(tweets)\n",
    "\n",
    "#Averlo?\n",
    "#print(codified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772104b3",
   "metadata": {},
   "source": [
    "Codificamos el objetivo, preprocesandolo previamente. Necesitamos dar valores ordinarios a los valores discretos originales, podríamos trabajar sobre la polarización entre -1 y 1 original pero crearía valores muy dispersos y así los unificamos, no tenemos infinitos posibles valores si no solamente 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4e86ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 4 ... 3 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Codificador para el objetivo\n",
    "cod_target = preprocessing.LabelEncoder()\n",
    "\n",
    "#Sacamos la columna sentiment del CSV de results\n",
    "codified_target = cod_target.fit_transform(tweets_results['sentiment'])\n",
    "\n",
    "#Printi printi\n",
    "print(codified_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89a1a7",
   "metadata": {},
   "source": [
    "Creamos los conjuntos de entrenamiento y de prueba cogiendo el 67% aprox para entrenamiento y el 30% para prueba y ver como de preciso es nuestro modelo. Pondremos una semilla concreta de _12345_ que mantendremos durante todo el código y así trabajar siemrpe con la misma randomización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3414af9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  (18411, 22865)\n",
      "Test data size:  (9069, 22865)\n"
     ]
    }
   ],
   "source": [
    "train_text, test_text, train_target, test_target = train_test_split(codified_text, codified_target, \n",
    "                                                                    random_state=12345, \n",
    "                                                                    test_size=0.33)\n",
    "\n",
    "print('Training data size: ', train_text.shape)\n",
    "print('Test data size: ', test_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422e55a",
   "metadata": {},
   "source": [
    "Aplicamos _TF-IDF (Term Frequency-Inverse Document Frequency)_ a los conjuntos para así darle la importancia que merecen a las palabras que más salgan, que más se repitan, a las menos comunes... Y así entrenar el modelo con pesos, minimizando la importancia de palabras poco frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19594851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tfidf data size:  (18411, 22865)\n",
      "Test tfidf data size:  (9069, 22865)\n"
     ]
    }
   ],
   "source": [
    "#Tf-idf\n",
    "\n",
    "#Creamos instancia vector tfidf\n",
    "vectorizer_tfidf = TfidfTransformer()\n",
    "\n",
    "train_text_tfidf = vectorizer_tfidf.fit_transform(train_text)\n",
    "\n",
    "test_text_tfidf = vectorizer_tfidf.transform(test_text)\n",
    "\n",
    "print('Training tfidf data size: ', train_text_tfidf.shape)\n",
    "print('Test tfidf data size: ', test_text_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e507b2",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Con los conjuntos de prueba y entrenamiento en tf-idf vamos a entrenar los modelos, empezamos por Naive Bayes. Usamos MultinomialNB porque tenemos un vector disperso de datos, funcionando bien para conjunto de datos enormes (como nuestro caso), para trabajar con CategoricalNB haría falta crear una matriz densa y eso puede dar problemas si el conjunto de datos es enorme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8f07221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5a7f4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos el modelo, con suavizado (alpha=1.0)\n",
    "modelNB = MultinomialNB(alpha=1.0)\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelNB.fit(train_text, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "736fcc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto, calculada con predict           0.7706472598963502\n",
      "Tasa de acierto, calculada con accuracy_score    0.7706472598963502\n"
     ]
    }
   ],
   "source": [
    "#Hacemos prediccion con los datos de prueba\n",
    "testNB = modelNB.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelNB.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48839da8",
   "metadata": {},
   "source": [
    "### k-Nn\n",
    "\n",
    "Al modelo kNn, k-n vecinos, elegiremos que n sea 5 por ser el estandar y el coseno porque es el mejor para comparar vectores que es, en definitiva, lo que tenemos, vectores dispersos con las _\"palabras\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6328e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b857a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelKNN = KNeighborsClassifier(n_neighbors=5, metric='cosine') #Usamos coseno porque es mejor para clasificar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1428485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(metric=&#x27;cosine&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(metric='cosine')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos el modelo con los parametros deseados\n",
    "modelKNN = KNeighborsClassifier(n_neighbors=5, metric='cosine') #Usamos coseno porque es mejor para clasificar texto\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelKNN.fit(train_text, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2ca6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto, calculada con predict           0.6572940787297387\n",
      "Tasa de acierto, calculada con accuracy_score    0.6572940787297387\n"
     ]
    }
   ],
   "source": [
    "#Hacemos prediccion con los datos de prueba\n",
    "testKNN = modelKNN.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelKNN.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testKNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904323c4",
   "metadata": {},
   "source": [
    "### Árboles de decisión.\n",
    "\n",
    "En este modelo hemos usado comparación cruzada, aunque solo mostramos dos de ellas por reducir el código innecesario. En una opción hemos usado el criterio de _Entropía_, con la profundidad del arbol _3_, y en otro hemos usado _Gini_ con profundidad _5_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7112ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7ea6a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=3, random_state=12345)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=3, random_state=12345)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=12345)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos modelo con arbol, profundidad 3, criterio Entropy\n",
    "modelTree = DecisionTreeClassifier(max_depth=3, criterion='entropy', random_state=12345)\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelTree.fit(train_text, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50985327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto, calculada con predict           0.5693020178630499\n",
      "Tasa de acierto, calculada con accuracy_score    0.5693020178630499\n"
     ]
    }
   ],
   "source": [
    "#Hacemos prediccion con los datos de prueba\n",
    "testTree = modelTree.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelTree.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90f31976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto, calculada con predict           0.608556621457713\n",
      "Tasa de acierto, calculada con accuracy_score    0.608556621457713\n"
     ]
    }
   ],
   "source": [
    "#Creamos modelo con arbol, profunidad 5, criterio Gini\n",
    "modelTree2 = DecisionTreeClassifier(max_depth=5, criterion='gini', random_state=12345)\n",
    ",\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelTree2.fit(train_text, train_target)\n",
    "\n",
    "#Hacemos prediccion con los datos de prueba\n",
    "testTree2 = modelTree2.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelTree2.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testTree2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbd329",
   "metadata": {},
   "source": [
    "Comprobamos que el árbol de decisión con criterio de Gini y profundidad 5 conseguimos la tasa de aciertos más alta, aunque no por demasiado.\n",
    "\n",
    "\n",
    "Invertimos los valores de los objetivos para comprobar los datos y ver como ha predecido.\n",
    "\n",
    "#### Comprobación de valores\n",
    "Vemos que el modelo con mejor tasa de aciertos es Naive Bayes, lo usaremos más adelante como el modelo elegido.\n",
    "Hacemos la inversa de los objetivos antes y después del modelo para comprobar cómo de acertado ha estado, aunque vemos que no demasiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f35b3d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutro', 'muy feliz', 'neutro', ..., 'contento', 'contento',\n",
       "       'neutro'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentimientos antes del modelo\n",
    "\n",
    "cod_target.inverse_transform(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92704ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutro', 'contento', 'neutro', ..., 'contento', 'contento',\n",
       "       'neutro'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentimientos despues del modelo\n",
    "\n",
    "cod_target.inverse_transform(testNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c46e5",
   "metadata": {},
   "source": [
    "## Analisis de tweets\n",
    "Hacemos scrapping de algun usuario de Twitter, algun influencer, y analizamos tras aplicar el modelo entrenado. Hemos creado un script para hacer scrapping directamente de la página de Twitter, con _selenium_ y _BeautifulSoup_. El script abre el perfil de Twitter del usuario indicado con una instancia de Chromium automatizada con _selenium_, espera 3 segundos y scrollea hasta el final de la página, cargando todos los tweets que va guardando temporalmente. Realiza ese mismo procedimiento las veces que sea necesario hasta que llegue al mínimo indicado, y los guarda en un archivo _csv_ personalizado. \n",
    "\n",
    "La lista de usuarios de los que obtenemos tweets y el numero de tweets que conseguimos se edita al fondo del script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17d8d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_tweets(username, amount=10):\n",
    "    all_the_tweets = []  # Una lista para almacenar todos los tweets de una página\n",
    "    driver = webdriver.Chrome()  # Requiere tener instalado el driver de Chrome\n",
    "    driver.get(f\"https://twitter.com/{username}\")\n",
    "    sleep_time = 3\n",
    "    time.sleep(sleep_time)  # Espera unos segundos para que se carguen los tweets\n",
    "\n",
    "    # Repetir el proceso hasta que tenga 100 tweets\n",
    "    while len(all_the_tweets) < amount:\n",
    "        # Desplazarse hacia abajo para cargar más tweets (opcional)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_time)  # Espera adicional para cargar más tweets si es necesario\n",
    "\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        tweets = soup.find_all(\"div\", attrs={\"data-testid\": \"tweetText\"})\n",
    "        all_the_tweets += tweets\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    if all_the_tweets:\n",
    "        # Obtener la ruta relativa al directorio del proyecto\n",
    "        script_directory = os.path.join(os.getcwd(), \"influencers\")\n",
    "        # crear el directorio si no existe\n",
    "        if not os.path.exists(script_directory):\n",
    "            os.makedirs(script_directory)\n",
    "            print(f\"Se ha creado el directorio {script_directory}\")\n",
    "        file_path = os.path.join(script_directory, f\"tweets_{username}.csv\")\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\", newline=\"\",) as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"text\"])  # Escribir la fila de encabezado\n",
    "            for tweet in all_the_tweets:\n",
    "                tweet_span = tweet.find_next(\"span\")\n",
    "                tweet_text = tweet_span.get_text(strip=True)\n",
    "                tweet_text = tweet_text.replace(\"\\n\", \" \")  # Eliminar los saltos de línea del contenido del tweet\n",
    "                writer.writerow([tweet_text])\n",
    "\n",
    "              #  print(tweet_text)\n",
    "              #  print(\"------------------------\")\n",
    "\n",
    "        print(f\"Los nuevos tweets se han guardado en {file_path}\")\n",
    "    else:\n",
    "        print(\"No se encontró el contenedor de tweets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e88bb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de usuarios a extraer tweets\n",
    "usernames=['cobratate','xQc','elonmusk','jk_rowling','pontifex','bbcnews','AlertWorld_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e58ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los nuevos tweets se han guardado en C:\\Users\\rafse\\Documents\\Proyecto IA\\Notebook\\influencers\\tweets_cobratate.csv\n",
      "Los nuevos tweets se han guardado en C:\\Users\\rafse\\Documents\\Proyecto IA\\Notebook\\influencers\\tweets_xQc.csv\n",
      "Los nuevos tweets se han guardado en C:\\Users\\rafse\\Documents\\Proyecto IA\\Notebook\\influencers\\tweets_elonmusk.csv\n",
      "Los nuevos tweets se han guardado en C:\\Users\\rafse\\Documents\\Proyecto IA\\Notebook\\influencers\\tweets_jk_rowling.csv\n",
      "Los nuevos tweets se han guardado en C:\\Users\\rafse\\Documents\\Proyecto IA\\Notebook\\influencers\\tweets_pontifex.csv\n"
     ]
    }
   ],
   "source": [
    "#Ejecución del script\n",
    "for user in usernames:\n",
    "    scrape_tweets(user, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f3f0d",
   "metadata": {},
   "source": [
    "Creamos un script para limpiar los tweets de los usuarios tal cual hicimos con el data set inicial, entrenamos el vectorizador con nuestro modelo y codificamos el texto, y predecimos según el modelo (Naive Bayes en nuestro caso). Luego representamos en un _pie chart_ los sentimientos de cada usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sentiments(username: str):\n",
    "    try:\n",
    "        user_tweetsCSV = pandas.read_csv(f\"../Notebook/influencers/tweets_{username}.csv\", header=0)\n",
    "        print(f'Analizando username --> {username}')\n",
    "        #print(user_tweetsCSV.shape)\n",
    "        user_tweets = user_tweetsCSV[['text']]\n",
    "\n",
    "        user_cleaned = clean_text(user_tweets)\n",
    "\n",
    "        # Codificamos los tweets para la predicción \n",
    "\n",
    "        tweets_user = user_cleaned['text']\n",
    "\n",
    "        # Convertimos las listas de palabras en cadenas de texto\n",
    "        tweets_user_str = [' '.join(tweet) for tweet in tweets_user]\n",
    "\n",
    "        vect.fit(tweets)\n",
    "\n",
    "        # Utilizamos el vectorizador previamente ajustado para transformar los tweets \n",
    "        codified_text_user = vect.transform(tweets_user_str)\n",
    "\n",
    "        userNB = modelNB.predict(codified_text_user)\n",
    "\n",
    "        tag, freq = numpy.unique(cod_target.inverse_transform(userNB), return_counts=True)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.pie(freq, labels=tag, autopct='%1.1f%%')\n",
    "        plt.title(f'Sentimientos de {username}')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for username '{username}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9082a28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mostrar pie chart con los sentimientos\n",
    "for user in usernames:\n",
    "    sentiments(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977c30d",
   "metadata": {},
   "source": [
    "#### Conclusión\n",
    "Ninguno de los usuarios analizados, algunos conocidos por ser personas tóxicas, personas bastante hater en general pero que no trasladan sus pensamientos directamente a sus tweets, o no usan palabras clasificadas como malas y por eso la mayoría aparecen como personas contentas y, en su mayoria, neutrales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
