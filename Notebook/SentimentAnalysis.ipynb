{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae511335",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia Artificial\n",
    "\n",
    "## Análisis de sentimientos\n",
    "\n",
    "Realizado por:\n",
    "- Alicia Sánchez Hossdorf\n",
    "- Rafael Segura Gómez\n",
    "\n",
    "Fecha: 26/05/2023\n",
    "\n",
    "Convocatoria de Junio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2343c24",
   "metadata": {},
   "source": [
    "<h1> 1. Recopilación de datos. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0958c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba71c79",
   "metadata": {},
   "source": [
    "Hemos descargado todos los tweets que necesitamos en un CSV. Por lo que debemos leer los datos del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc99f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento.csv\", header=0)\n",
    "tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento_small.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874309a0",
   "metadata": {},
   "source": [
    "Para saber cuantos tweets tenemos, miraremos cuantas filas tiene el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b203ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweetsCSV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3ada5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                I`d have responded, if I were going   neutral  \n",
       "1                                           Sooo SAD  negative  \n",
       "2                                        bullying me  negative  \n",
       "3                                     leave me alone  negative  \n",
       "4                                      Sons of ****,  negative  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                fun  positive  \n",
       "7                                         Soooo high   neutral  \n",
       "8                                        Both of you   neutral  \n",
       "9                       Wow... u just became cooler.  positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweetsCSV.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eee3ec",
   "metadata": {},
   "source": [
    "Como solo necesitamos la columna text, que es donde viene los tweets que necesitamos, haremos una seleccion de dicha columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d919204",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweetsCSV[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670f9a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test from the LG enV2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S`ok, trying to plot alternatives as we speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i`ve been sick for the past few days  and thus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hes just not that into you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0                 I`d have responded, if I were going\n",
       "1       Sooo SAD I will miss you here in San Diego!!!\n",
       "2                           my boss is bullying me...\n",
       "3                      what interview! leave me alone\n",
       "4    Sons of ****, why couldn`t they put them on t...\n",
       "5   http://www.dothebouncy.com/smf - some shameles...\n",
       "6   2am feedings for the baby are fun when he is a...\n",
       "7                                          Soooo high\n",
       "8                                         Both of you\n",
       "9    Journey!? Wow... u just became cooler.  hehe....\n",
       "10   as much as i love to be hopeful, i reckon the...\n",
       "11  I really really like the song Love Story by Ta...\n",
       "12       My Sharpie is running DANGERously low on ink\n",
       "13  i want to go to music tonight but i lost my vo...\n",
       "14                         test test from the LG enV2\n",
       "15                              Uh oh, I am sunburned\n",
       "16   S`ok, trying to plot alternatives as we speak...\n",
       "17  i`ve been sick for the past few days  and thus...\n",
       "18         is back home now      gonna miss every one\n",
       "19                         Hes just not that into you"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd330f5a",
   "metadata": {},
   "source": [
    "<h1> 2. Eliminar las palabras que no aportan información </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7cc2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6bcf0bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003ce35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('punkt', download_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b52f1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doing', 'why', 'yourselves', 'if', 'up', \"wasn't\", 'weren', 'for', 'does', 'having', 'below', 'in', 'when', 'our', 'we', 'during', 'her', 'until', 'he', 'theirs', 'after', 'down', \"isn't\", 'some', 'your', 'didn', 'to', 'here', 'haven', 'ourselves', 'from', 'did', 'are', 'mightn', \"hadn't\", 'other', \"needn't\", 'shan', 'ma', 'mustn', 'what', 'you', 'm', \"mightn't\", \"that'll\", 'both', 'been', 'aren', 'it', \"it's\", 'no', 'hasn', 'between', 'just', 'with', 'through', 'was', 'all', 'only', 'shouldn', 'herself', 'the', \"weren't\", \"you've\", 'be', 'under', 've', 's', 'had', 'doesn', 'about', 'have', \"don't\", \"you'll\", 'his', \"hasn't\", 'by', 'she', 'himself', 'not', 'above', 'needn', 'do', 'being', 'where', 'yours', 'don', 'wouldn', \"doesn't\", 'wasn', 'there', 'me', 'them', 'nor', 'they', 'how', 'more', 'against', 'at', 'as', 'then', 'ain', 'this', 'who', 'will', 'were', \"you'd\", 'my', 'can', \"didn't\", 'o', 'ours', 'these', 'him', 'or', 'because', 'its', \"you're\", \"she's\", 'too', 'yourself', 'and', \"shouldn't\", 'themselves', 'of', 'so', \"mustn't\", 'own', 'off', 'should', 're', 'couldn', 'which', 'isn', 'very', 'an', 'than', 'myself', 'that', 'whom', 'd', 't', 'a', 'hadn', 'won', 'before', 'each', \"shan't\", 'few', 'but', 'again', 'on', 'such', 'out', 'itself', 'further', 'hers', \"should've\", 'into', 'most', 'now', \"haven't\", 'while', 'over', 'y', 'same', 'any', \"couldn't\", 'once', 'i', \"won't\", 'their', \"wouldn't\", 'has', 'is', 'those', 'll', 'am', \"aren't\"}\n"
     ]
    }
   ],
   "source": [
    "sWords = set(stopwords.words('english'))\n",
    "print(sWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698cfa1",
   "metadata": {},
   "source": [
    "Crear una función clean_text para eliminar las palabras comunes y poco informativas; eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño; y que utilice técnicas de lematización (stemming) y tokenización para reducir las palabras a su forma base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c5b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar las stop words y tokenizado.\n",
    "\n",
    "def delete_stopWords(tweet):\n",
    "    tweetLow = tweet.lower()\n",
    "    tweetWords = word_tokenize(tweetLow)\n",
    "    tweetFiltered = []\n",
    "        \n",
    "    for w in tweetWords:\n",
    "        if w not in sWords:\n",
    "            tweetFiltered.append(w)\n",
    "    return tweetFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "301d7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion es para eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño.\n",
    "\n",
    "def delete_otherElements(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"http[^\\s]+\",\"\",tweet)\n",
    "    tweetFiltered = re.sub(r\"[-()\\\"#/@;:<>{}+=*~|.!?,]\", \"\", tweet)\n",
    "\n",
    "    return tweetFiltered\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb9f7",
   "metadata": {},
   "source": [
    "## Función para limpiar el text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe91be",
   "metadata": {},
   "source": [
    "Con los tweets tokenizados, lematizados y sin stop words lo pasamos a lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea662b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(Tweets): \n",
    "    res = Tweets.copy()\n",
    "    i = 0;\n",
    "    #Iniciar SnowBall Stemmer con idioma ingles\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    while i<len(tweets):\n",
    "        tweet_cleaned = []\n",
    "        \n",
    "        clean_t = delete_otherElements(Tweets['text'].get(i))\n",
    "        \n",
    "        #__Para corregir gramaticalmente descomentar la siguiente linea__\n",
    "        #clean_t = str(TextBlob(clean_t).correct())\n",
    "        \n",
    "        tNoStopWords = delete_stopWords(clean_t)\n",
    "        #Esto es para la lematización de la frase. \n",
    "        for w in tNoStopWords:\n",
    "            if w != \"`\":\n",
    "                tweet_cleaned.append(stemmer.stem(w))\n",
    "        res.at[i,'text'] = tweet_cleaned\n",
    "        \n",
    "        i+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75694d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned = clean_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6ef69c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text\n",
      "0                                       [respond, go]\n",
      "1                       [sooo, sad, miss, san, diego]\n",
      "2                                       [boss, bulli]\n",
      "3                             [interview, leav, alon]\n",
      "4                 [son, put, releas, alreadi, bought]\n",
      "5       [shameless, plug, best, ranger, forum, earth]\n",
      "6                  [2am, feed, babi, fun, smile, coo]\n",
      "7                                       [soooo, high]\n",
      "8                                                  []\n",
      "9     [journey, wow, u, becam, cooler, hehe, possibl]\n",
      "10  [much, love, hope, reckon, chanc, minim, p, ne...\n",
      "11  [realli, realli, like, song, love, stori, tayl...\n",
      "12                    [sharpi, run, danger, low, ink]\n",
      "13             [want, go, music, tonight, lost, voic]\n",
      "14                             [test, test, lg, env2]\n",
      "15                                  [uh, oh, sunburn]\n",
      "16               [ok, tri, plot, altern, speak, sigh]\n",
      "17  [sick, past, day, thus, hair, look, wierd, did...\n",
      "18            [back, home, gon, na, miss, everi, one]\n",
      "19                                              [hes]\n"
     ]
    }
   ],
   "source": [
    "print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46cfdf",
   "metadata": {},
   "source": [
    "### A partir de aquí, hasta el etiquetado de datos, solamente es prueba de corrección gramatical, se puede ignorar, son pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb9cbb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  \\\n",
      "0                                       [respond, go]   \n",
      "1                       [sooo, sad, miss, san, diego]   \n",
      "2                                       [boss, bulli]   \n",
      "3                             [interview, leav, alon]   \n",
      "4                 [son, put, releas, alreadi, bought]   \n",
      "5       [shameless, plug, best, ranger, forum, earth]   \n",
      "6                  [2am, feed, babi, fun, smile, coo]   \n",
      "7                                       [soooo, high]   \n",
      "8                                                  []   \n",
      "9     [journey, wow, u, becam, cooler, hehe, possibl]   \n",
      "10  [much, love, hope, reckon, chanc, minim, p, ne...   \n",
      "11  [realli, realli, like, song, love, stori, tayl...   \n",
      "12                    [sharpi, run, danger, low, ink]   \n",
      "13             [want, go, music, tonight, lost, voic]   \n",
      "14                             [test, test, lg, env2]   \n",
      "15                                  [uh, oh, sunburn]   \n",
      "16               [ok, tri, plot, altern, speak, sigh]   \n",
      "17  [sick, past, day, thus, hair, look, wierd, did...   \n",
      "18            [back, home, gon, na, miss, everi, one]   \n",
      "19                                              [hes]   \n",
      "\n",
      "                                       text_corregido  \n",
      "0                                          respond go  \n",
      "1                             soon sad miss san diego  \n",
      "2                                           boss bull  \n",
      "3                               interview leave along  \n",
      "4                      son put release already bought  \n",
      "5              shameless plug best ranger forum earth  \n",
      "6                          am feed baby fun smile too  \n",
      "7                                           soon high  \n",
      "8                                                      \n",
      "9           journey now u became cooper here possible  \n",
      "10  much love hope reckon chance minims p never on...  \n",
      "11    really really like song love story taylor swift  \n",
      "12                           sharp run danger low ink  \n",
      "13                   want go music tonight lost voice  \n",
      "14                                  test test ll envy  \n",
      "15                                     up oh sunburnt  \n",
      "16                       ok try plot alter speak sigh  \n",
      "17  sick past day thus hair look wired didn hat wo...  \n",
      "18                     back home on na miss every one  \n",
      "19                                                 he  \n"
     ]
    }
   ],
   "source": [
    "# Corrige el dataframe y muestra ambas columnas, corregido y original, no modifica variables originales\n",
    "tweets_cleaned['text_corregido'] = tweets_cleaned['text'].apply(lambda x: str(TextBlob(\" \".join(x)).correct()))\n",
    "\n",
    "\n",
    "print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b57f90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_corregido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>respond go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soon sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boss bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview leave along</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>son put release already bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>you ride one catch one summer til pop open one '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>good gorjuz yea no ask yesterday the hospital ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>ok pop say hi check thing probably head gutta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>now really mission family today added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>source say</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_corregido\n",
       "0                                          respond go\n",
       "1                             soon sad miss san diego\n",
       "2                                           boss bull\n",
       "3                               interview leave along\n",
       "4                      son put release already bought\n",
       "..                                                ...\n",
       "73   you ride one catch one summer til pop open one '\n",
       "74  good gorjuz yea no ask yesterday the hospital ...\n",
       "75  ok pop say hi check thing probably head gutta ...\n",
       "76              now really mission family today added\n",
       "77                                         source say\n",
       "\n",
       "[78 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra el dataframe con los textos corregidos\n",
    "tweets_cleaned[['text_corregido']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1b3d",
   "metadata": {},
   "source": [
    "## Etiquetado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00e10b",
   "metadata": {},
   "source": [
    "Documentar:\n",
    "- !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1acaa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01e1847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_text(text):\n",
    "    tweet = ' '.join(text)  # Convertir la lista de palabras en una cadena de text\n",
    "    blob = TextBlob(tweet) #Convierte los tweets a tipo Blob para aplicarle la libreria\n",
    "    tag = blob.sentiment.polarity #Polaridad entre -1 y 1, de peor a mejor respectivamente\n",
    "    return text, tag\n",
    "\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "\n",
    "#Escritura en un archivo CSV nuevo\n",
    "with open(csv_path, 'w', newline='',encoding='utf-8-sig') as csv_file: #Forzamos formato utf-8, otro da problemas\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['text', 'tag', 'sentiment']) #Crea columna sentiment también\n",
    "    \n",
    "    if len(tweets_cleaned) == 0:  #Checkea si está vacío\n",
    "        print(\"Nothing to tag, espabila notas.\")\n",
    "        exit()\n",
    "\n",
    "    for _, row in tweets_cleaned.iterrows(): #Usamos iterrows porque tweets_cleaned es de tipo dataframe\n",
    "       # if row['text']:  # Verificar si el campo 'text' no está vacío\n",
    "            result = tag_text(row['text'])\n",
    "            result = (*result, '')  # Añadir una cadena vacía para la columna \"sentiment\"\n",
    "            writer.writerow(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71e8c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorizando los tags a valores discretos\n",
    "def etiqueta_codificada(etiqueta):\n",
    "    if etiqueta >= -1 and etiqueta < -0.5:\n",
    "        return \"hater\"\n",
    "    elif etiqueta >= -0.5 and etiqueta < 0:\n",
    "        return \"molesto\"\n",
    "    elif etiqueta == 0:\n",
    "        return \"neutro\"\n",
    "    elif etiqueta > 0 and etiqueta <= 0.5:\n",
    "        return \"contento\"\n",
    "    elif etiqueta > 0.5 and etiqueta <= 1:\n",
    "        return \"muy feliz\"\n",
    "    else:\n",
    "        return \"null\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36382b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actualizar la columna sentiment del csv\n",
    "# Leer el archivo CSV\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "df = pandas.read_csv(csv_path)\n",
    "\n",
    "# Aplicar la función etiqueta_codificada para asignar los valores de tag a sentiment\n",
    "df['sentiment'] = df['tag'].apply(etiqueta_codificada)\n",
    "\n",
    "\n",
    "\n",
    "# Guardar el archivo CSV actualizado\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98810428",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_results = pandas.read_csv(\"../Notebook/Tweets/resultados.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d2fc0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ea702d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['respond', 'go']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['sooo', 'sad', 'miss', 'san', 'diego']</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['boss', 'bulli']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['interview', 'leav', 'alon']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['son', 'put', 'releas', 'alreadi', 'bought']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['shameless', 'plug', 'best', 'ranger', 'forum...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>muy feliz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['2am', 'feed', 'babi', 'fun', 'smile', 'coo']</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['soooo', 'high']</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['journey', 'wow', 'u', 'becam', 'cooler', 'he...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>['much', 'love', 'hope', 'reckon', 'chanc', 'm...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>['realli', 'realli', 'like', 'song', 'love', '...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>['sharpi', 'run', 'danger', 'low', 'ink']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>['want', 'go', 'music', 'tonight', 'lost', 'vo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>['test', 'test', 'lg', 'env2']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>['uh', 'oh', 'sunburn']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>['ok', 'tri', 'plot', 'altern', 'speak', 'sigh']</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>['sick', 'past', 'day', 'thus', 'hair', 'look'...</td>\n",
       "      <td>-0.482143</td>\n",
       "      <td>molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>['back', 'home', 'gon', 'na', 'miss', 'everi',...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>['hes']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text       tag  sentiment\n",
       "0                                   ['respond', 'go']  0.000000     neutro\n",
       "1             ['sooo', 'sad', 'miss', 'san', 'diego'] -0.500000    molesto\n",
       "2                                   ['boss', 'bulli']  0.000000     neutro\n",
       "3                       ['interview', 'leav', 'alon']  0.000000     neutro\n",
       "4       ['son', 'put', 'releas', 'alreadi', 'bought']  0.000000     neutro\n",
       "5   ['shameless', 'plug', 'best', 'ranger', 'forum...  1.000000  muy feliz\n",
       "6      ['2am', 'feed', 'babi', 'fun', 'smile', 'coo']  0.300000   contento\n",
       "7                                   ['soooo', 'high']  0.160000   contento\n",
       "8                                                  []  0.000000     neutro\n",
       "9   ['journey', 'wow', 'u', 'becam', 'cooler', 'he...  0.100000   contento\n",
       "10  ['much', 'love', 'hope', 'reckon', 'chanc', 'm...  0.500000   contento\n",
       "11  ['realli', 'realli', 'like', 'song', 'love', '...  0.500000   contento\n",
       "12          ['sharpi', 'run', 'danger', 'low', 'ink']  0.000000     neutro\n",
       "13  ['want', 'go', 'music', 'tonight', 'lost', 'vo...  0.000000     neutro\n",
       "14                     ['test', 'test', 'lg', 'env2']  0.000000     neutro\n",
       "15                            ['uh', 'oh', 'sunburn']  0.000000     neutro\n",
       "16   ['ok', 'tri', 'plot', 'altern', 'speak', 'sigh']  0.500000   contento\n",
       "17  ['sick', 'past', 'day', 'thus', 'hair', 'look'... -0.482143    molesto\n",
       "18  ['back', 'home', 'gon', 'na', 'miss', 'everi',...  0.000000     neutro\n",
       "19                                            ['hes']  0.000000     neutro"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweets_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6e857",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "Necesitamos separar los datos que tenemos en conjunto de entrenamiento y conjunto de pruebas, para probar que funciona bien e ir entrenando el modelo.\n",
    "\n",
    "A partir de aqui, pruebitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa8bef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e35a92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cojo la columna text\n",
    "tweets = tweets_results['text']\n",
    "\n",
    "#Creamos el vector\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#Vectorizamos los tweets\n",
    "codified_text = vect.fit_transform(tweets)\n",
    "\n",
    "#Averlo?\n",
    "#print(codified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1ea348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 4 4 4 3 0 0 4 0 0 0 4 4 4 4 0 2 4 4 4 0 0 4 2 0 4 4 0 4 0 4 0 0 0 4 2\n",
      " 0 4 0 4 0 4 2 0 0 2 4 2 0 1 4 4 2 4 4 2 4 4 0 2 0 4 0 4 4 3 0 4 4 4 3 4 4\n",
      " 3 0 0 4]\n"
     ]
    }
   ],
   "source": [
    "#Codificador para el objetivo\n",
    "cod_target = preprocessing.LabelEncoder()\n",
    "\n",
    "#Sacamos la columna sentiment del CSV de results\n",
    "codified_target = cod_target.fit_transform(tweets_results['sentiment'])\n",
    "\n",
    "#Printi printi\n",
    "print(codified_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "056fce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  (52, 392)\n",
      "Test data size:  (26, 392)\n"
     ]
    }
   ],
   "source": [
    "train_text, test_text, train_target, test_target = train_test_split(codified_text, codified_target, \n",
    "                                                                    random_state=42, \n",
    "                                                                    test_size=0.33)\n",
    "\n",
    "print('Training data size: ', train_text.shape)\n",
    "print('Test data size: ', test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6d629e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tfidf data size:  (52, 392)\n",
      "Test tfidf data size:  (26, 392)\n"
     ]
    }
   ],
   "source": [
    "#Tf-idf\n",
    "\n",
    "#Creamos instancia vector tfidf\n",
    "vectorizer_tfidf = TfidfTransformer()\n",
    "\n",
    "train_text_tfidf = vectorizer_tfidf.fit_transform(train_text)\n",
    "\n",
    "test_text_tfidf = vectorizer_tfidf.transform(test_text)\n",
    "\n",
    "print('Training tfidf data size: ', train_text_tfidf.shape)\n",
    "print('Test tfidf data size: ', test_text_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a525d",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Con los conjuntos de prueba y entrenamiento en tf-idf vamos a entrenar los modelos, empezamos por Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5fe3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "601ac9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(force_alpha=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d394bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_text_tfidf, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535526a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = model.predict(test_prueba_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
