{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae511335",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia Artificial\n",
    "\n",
    "## Análisis de sentimientos\n",
    "\n",
    "Realizado por:\n",
    "- Alicia Sánchez Hossdorf\n",
    "- Rafael Segura Gómez\n",
    "\n",
    "Fecha: 26/05/2023\n",
    "\n",
    "Convocatoria de Junio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2343c24",
   "metadata": {},
   "source": [
    "<h1> 1. Recopilación de datos. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0958c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba71c79",
   "metadata": {},
   "source": [
    "Hemos descargado todos los tweets que necesitamos en un CSV. Por lo que debemos leer los datos del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc99f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento.csv\", header=0)\n",
    "tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento_small.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874309a0",
   "metadata": {},
   "source": [
    "Para saber cuantos tweets tenemos, miraremos cuantas filas tiene el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82b203ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweetsCSV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b3ada5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                I`d have responded, if I were going   neutral  \n",
       "1                                           Sooo SAD  negative  \n",
       "2                                        bullying me  negative  \n",
       "3                                     leave me alone  negative  \n",
       "4                                      Sons of ****,  negative  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                fun  positive  \n",
       "7                                         Soooo high   neutral  \n",
       "8                                        Both of you   neutral  \n",
       "9                       Wow... u just became cooler.  positive  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweetsCSV.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eee3ec",
   "metadata": {},
   "source": [
    "Como solo necesitamos la columna text, que es donde viene los tweets que necesitamos, haremos una seleccion de dicha columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d919204",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweetsCSV[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "670f9a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test from the LG enV2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S`ok, trying to plot alternatives as we speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i`ve been sick for the past few days  and thus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hes just not that into you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0                 I`d have responded, if I were going\n",
       "1       Sooo SAD I will miss you here in San Diego!!!\n",
       "2                           my boss is bullying me...\n",
       "3                      what interview! leave me alone\n",
       "4    Sons of ****, why couldn`t they put them on t...\n",
       "5   http://www.dothebouncy.com/smf - some shameles...\n",
       "6   2am feedings for the baby are fun when he is a...\n",
       "7                                          Soooo high\n",
       "8                                         Both of you\n",
       "9    Journey!? Wow... u just became cooler.  hehe....\n",
       "10   as much as i love to be hopeful, i reckon the...\n",
       "11  I really really like the song Love Story by Ta...\n",
       "12       My Sharpie is running DANGERously low on ink\n",
       "13  i want to go to music tonight but i lost my vo...\n",
       "14                         test test from the LG enV2\n",
       "15                              Uh oh, I am sunburned\n",
       "16   S`ok, trying to plot alternatives as we speak...\n",
       "17  i`ve been sick for the past few days  and thus...\n",
       "18         is back home now      gonna miss every one\n",
       "19                         Hes just not that into you"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd330f5a",
   "metadata": {},
   "source": [
    "<h1> 2. Eliminar las palabras que no aportan información </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7cc2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6bcf0bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "003ce35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('punkt', download_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b52f1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'having', 'any', 'been', 'through', 'we', 'or', 'there', 'ourselves', 'hasn', 'as', 'nor', 'each', 'are', 'again', \"she's\", 'wouldn', 'only', \"wouldn't\", 'did', 'to', 'isn', 'be', 'by', 'into', \"wasn't\", 'those', 'this', 'i', 'ours', 'my', 've', \"isn't\", \"it's\", 'on', 'out', 'mightn', \"mightn't\", \"weren't\", \"hasn't\", 'below', 'myself', 'very', 'because', \"won't\", 'do', 'so', 'after', 'during', 'of', 'here', 'should', 'himself', 'your', 'off', 'against', 'between', 'up', 'same', 'most', 'theirs', 'her', 'were', 'then', 'doing', \"needn't\", \"you're\", 'its', 'has', 'ma', 'in', 'll', 'she', 'above', \"doesn't\", 'but', 'am', 'weren', 'further', 'few', 'just', 'if', 'that', 'you', 'which', 'where', \"don't\", 'wasn', 'and', 'our', 'who', \"shan't\", 'whom', 'about', \"aren't\", 'the', \"hadn't\", 'ain', 'what', 'all', 'needn', 'at', 'o', 'until', 'haven', 'once', 'both', 'such', 'm', 'yourselves', \"that'll\", 'yourself', 'while', 'own', 'themselves', 'aren', \"didn't\", 'down', 'over', 'too', 'a', 't', 'y', 'me', 'will', 'yours', 'it', 'than', 'didn', 'itself', 'was', 'from', 'for', 'no', 'he', 'have', 'can', 'other', 'does', \"shouldn't\", 'with', 'how', 'not', 'shouldn', 'under', 'shan', 'hadn', \"couldn't\", 'being', 'an', 'mustn', 'him', \"you'll\", 'why', 'their', 'is', 's', 'hers', 'doesn', 'herself', 'had', 'more', 'them', 'they', 'some', \"mustn't\", 'don', 'his', 'couldn', \"you'd\", 'now', 'when', 're', \"you've\", \"should've\", 'won', 'these', 'before', 'd', \"haven't\"}\n"
     ]
    }
   ],
   "source": [
    "sWords = set(stopwords.words('english'))\n",
    "print(sWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698cfa1",
   "metadata": {},
   "source": [
    "Crear una función clean_text para eliminar las palabras comunes y poco informativas; eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño; y que utilice técnicas de lematización (stemming) y tokenización para reducir las palabras a su forma base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2a09f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar las stop words y tokenizado.\n",
    "\n",
    "def delete_stopWords(tweet):\n",
    "    tweetLow = tweet.lower()\n",
    "    tweetWords = word_tokenize(tweetLow)\n",
    "    tweetFiltered = []\n",
    "        \n",
    "    for w in tweetWords:\n",
    "        if w not in sWords:\n",
    "            tweetFiltered.append(w)\n",
    "    return tweetFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "301d7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion es para eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño.\n",
    "\n",
    "def delete_otherElements(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"http[^\\s]+\",\"\",tweet)\n",
    "    tweetFiltered = re.sub(r\"[-()\\\"#/@;:<>{}+=*~|.!?,]\", \"\", tweet)\n",
    "\n",
    "    return tweetFiltered\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb9f7",
   "metadata": {},
   "source": [
    "## Función para limpiar el text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe91be",
   "metadata": {},
   "source": [
    "Con los tweets tokenizados, lematizados y sin stop words lo pasamos a lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ea662b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(Tweets): \n",
    "    res = Tweets.copy()\n",
    "    i = 0;\n",
    "    #Iniciar SnowBall Stemmer con idioma ingles\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    while i<len(tweets):\n",
    "        tweet_cleaned = []\n",
    "        \n",
    "        clean_t = delete_otherElements(Tweets['text'].get(i))\n",
    "        \n",
    "        #__Para corregir gramaticalmente descomentar la siguiente linea__\n",
    "        #clean_t = str(TextBlob(clean_t).correct())\n",
    "        \n",
    "        tNoStopWords = delete_stopWords(clean_t)\n",
    "        #Esto es para la lematización de la frase. \n",
    "        for w in tNoStopWords:\n",
    "            if w != \"`\":\n",
    "                tweet_cleaned.append(stemmer.stem(w))\n",
    "        res.at[i,'text'] = tweet_cleaned\n",
    "        \n",
    "        i+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75694d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned = clean_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6ef69c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text\n",
      "0                                       [respond, go]\n",
      "1                       [sooo, sad, miss, san, diego]\n",
      "2                                       [boss, bulli]\n",
      "3                             [interview, leav, alon]\n",
      "4                 [son, put, releas, alreadi, bought]\n",
      "5       [shameless, plug, best, ranger, forum, earth]\n",
      "6                  [2am, feed, babi, fun, smile, coo]\n",
      "7                                       [soooo, high]\n",
      "8                                                  []\n",
      "9     [journey, wow, u, becam, cooler, hehe, possibl]\n",
      "10  [much, love, hope, reckon, chanc, minim, p, ne...\n",
      "11  [realli, realli, like, song, love, stori, tayl...\n",
      "12                    [sharpi, run, danger, low, ink]\n",
      "13             [want, go, music, tonight, lost, voic]\n",
      "14                             [test, test, lg, env2]\n",
      "15                                  [uh, oh, sunburn]\n",
      "16               [ok, tri, plot, altern, speak, sigh]\n",
      "17  [sick, past, day, thus, hair, look, wierd, did...\n",
      "18            [back, home, gon, na, miss, everi, one]\n",
      "19                                              [hes]\n"
     ]
    }
   ],
   "source": [
    "print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867d2a3",
   "metadata": {},
   "source": [
    "### A partir de aquí, hasta el etiquetado de datos, solamente es prueba de corrección gramatical, se puede ignorar, son pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ec5d067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  \\\n",
      "0                                       [respond, go]   \n",
      "1                       [sooo, sad, miss, san, diego]   \n",
      "2                                       [boss, bulli]   \n",
      "3                             [interview, leav, alon]   \n",
      "4                 [son, put, releas, alreadi, bought]   \n",
      "5       [shameless, plug, best, ranger, forum, earth]   \n",
      "6                  [2am, feed, babi, fun, smile, coo]   \n",
      "7                                       [soooo, high]   \n",
      "8                                                  []   \n",
      "9     [journey, wow, u, becam, cooler, hehe, possibl]   \n",
      "10  [much, love, hope, reckon, chanc, minim, p, ne...   \n",
      "11  [realli, realli, like, song, love, stori, tayl...   \n",
      "12                    [sharpi, run, danger, low, ink]   \n",
      "13             [want, go, music, tonight, lost, voic]   \n",
      "14                             [test, test, lg, env2]   \n",
      "15                                  [uh, oh, sunburn]   \n",
      "16               [ok, tri, plot, altern, speak, sigh]   \n",
      "17  [sick, past, day, thus, hair, look, wierd, did...   \n",
      "18            [back, home, gon, na, miss, everi, one]   \n",
      "19                                              [hes]   \n",
      "\n",
      "                                       text_corregido  \n",
      "0                                          respond go  \n",
      "1                             soon sad miss san diego  \n",
      "2                                           boss bull  \n",
      "3                               interview leave along  \n",
      "4                      son put release already bought  \n",
      "5              shameless plug best ranger forum earth  \n",
      "6                          am feed baby fun smile too  \n",
      "7                                           soon high  \n",
      "8                                                      \n",
      "9           journey now u became cooper here possible  \n",
      "10  much love hope reckon chance minims p never on...  \n",
      "11    really really like song love story taylor swift  \n",
      "12                           sharp run danger low ink  \n",
      "13                   want go music tonight lost voice  \n",
      "14                                  test test ll envy  \n",
      "15                                     up oh sunburnt  \n",
      "16                       ok try plot alter speak sigh  \n",
      "17  sick past day thus hair look wired didn hat wo...  \n",
      "18                     back home on na miss every one  \n",
      "19                                                 he  \n"
     ]
    }
   ],
   "source": [
    "# Corrige el dataframe y muestra ambas columnas, corregido y original, no modifica variables originales\n",
    "tweets_cleaned['text_corregido'] = tweets_cleaned['text'].apply(lambda x: str(TextBlob(\" \".join(x)).correct()))\n",
    "\n",
    "\n",
    "print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e60cdd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_corregido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>respond go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soon sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boss bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview leave along</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>son put release already bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>you ride one catch one summer til pop open one '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>good gorjuz yea no ask yesterday the hospital ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>ok pop say hi check thing probably head gutta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>now really mission family today added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>source say</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_corregido\n",
       "0                                          respond go\n",
       "1                             soon sad miss san diego\n",
       "2                                           boss bull\n",
       "3                               interview leave along\n",
       "4                      son put release already bought\n",
       "..                                                ...\n",
       "73   you ride one catch one summer til pop open one '\n",
       "74  good gorjuz yea no ask yesterday the hospital ...\n",
       "75  ok pop say hi check thing probably head gutta ...\n",
       "76              now really mission family today added\n",
       "77                                         source say\n",
       "\n",
       "[78 rows x 1 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra el dataframe con los textos corregidos\n",
    "tweets_cleaned[['text_corregido']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1b3d",
   "metadata": {},
   "source": [
    "## Etiquetado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00e10b",
   "metadata": {},
   "source": [
    "Documentar:\n",
    "- !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1acaa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "01e1847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_text(text):\n",
    "    tweet = ' '.join(text)  # Convertir la lista de palabras en una cadena de text\n",
    "    blob = TextBlob(tweet) #Convierte los tweets a tipo Blob para aplicarle la libreria\n",
    "    tag = blob.sentiment.polarity #Polaridad entre -1 y 1, de peor a mejor respectivamente\n",
    "    return tweet, tag\n",
    "\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "\n",
    "#Escritura en un archivo CSV nuevo\n",
    "with open(csv_path, 'w', newline='',encoding='utf-8-sig') as csv_file: #Forzamos formato utf-8, otro da problemas\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['text', 'tag', 'sentiment']) #Crea columna sentiment también\n",
    "    \n",
    "    if len(tweets_cleaned) == 0:  #Checkea si está vacío\n",
    "        print(\"Nothing to tag, espabila notas.\")\n",
    "        exit()\n",
    "\n",
    "    for _, row in tweets_cleaned.iterrows(): #Usamos iterrows porque tweets_cleaned es de tipo dataframe\n",
    "        result = tag_text(row['text'])\n",
    "        result = (*result, '')  # Añadir una cadena vacía para la columna \"sentiment\"\n",
    "        writer.writerow(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9bcaeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorizando los tags a valores discretos\n",
    "def etiqueta_codificada(etiqueta):\n",
    "    if etiqueta >= -1 and etiqueta < -0.5:\n",
    "        return \"hater\"\n",
    "    elif etiqueta >= -0.5 and etiqueta < 0:\n",
    "        return \"molesto\"\n",
    "    elif etiqueta == 0:\n",
    "        return \"neutro\"\n",
    "    elif etiqueta > 0 and etiqueta <= 0.5:\n",
    "        return \"contento\"\n",
    "    elif etiqueta > 0.5 and etiqueta <= 1:\n",
    "        return \"muy feliz\"\n",
    "    else:\n",
    "        return \"null\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b50ddda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actualizar la columna sentiment del csv\n",
    "# Leer el archivo CSV\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "df = pandas.read_csv(csv_path)\n",
    "\n",
    "# Aplicar la función etiqueta_codificada para asignar los valores de tag a sentiment\n",
    "df['sentiment'] = df['tag'].apply(etiqueta_codificada)\n",
    "\n",
    "# Guardar el archivo CSV actualizado\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "98810428",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_results = pandas.read_csv(\"../Notebook/Tweets/resultados.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1d2fc0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1ea702d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>respond go</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boss bulli</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview leav alon</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>son put releas alreadi bought</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shameless plug best ranger forum earth</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>muy feliz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feed babi fun smile coo</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>soooo high</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>journey wow u becam cooler hehe possibl</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>much love hope reckon chanc minim p never gon ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>realli realli like song love stori taylor swift</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sharpi run danger low ink</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>want go music tonight lost voic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test lg env2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>uh oh sunburn</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ok tri plot altern speak sigh</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sick past day thus hair look wierd didnt hat w...</td>\n",
       "      <td>-0.482143</td>\n",
       "      <td>molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>back home gon na miss everi one</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text       tag  sentiment\n",
       "0                                          respond go  0.000000     neutro\n",
       "1                             sooo sad miss san diego -0.500000    molesto\n",
       "2                                          boss bulli  0.000000     neutro\n",
       "3                                 interview leav alon  0.000000     neutro\n",
       "4                       son put releas alreadi bought  0.000000     neutro\n",
       "5              shameless plug best ranger forum earth  1.000000  muy feliz\n",
       "6                         2am feed babi fun smile coo  0.300000   contento\n",
       "7                                          soooo high  0.160000   contento\n",
       "8                                                 NaN  0.000000     neutro\n",
       "9             journey wow u becam cooler hehe possibl  0.100000   contento\n",
       "10  much love hope reckon chanc minim p never gon ...  0.500000   contento\n",
       "11    realli realli like song love stori taylor swift  0.500000   contento\n",
       "12                          sharpi run danger low ink  0.000000     neutro\n",
       "13                    want go music tonight lost voic  0.000000     neutro\n",
       "14                                  test test lg env2  0.000000     neutro\n",
       "15                                      uh oh sunburn  0.000000     neutro\n",
       "16                      ok tri plot altern speak sigh  0.500000   contento\n",
       "17  sick past day thus hair look wierd didnt hat w... -0.482143    molesto\n",
       "18                    back home gon na miss everi one  0.000000     neutro\n",
       "19                                                hes  0.000000     neutro"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweets_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e98a20",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "Necesitamos separar los datos que tenemos en conjunto de entrenamiento y conjunto de pruebas, para probar que funciona bien e ir entrenando el modelo.\n",
    "\n",
    "A partir de aqui estoy de pruebas, nada de ésto funciona, aunque algunas cosas hacen cosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "64487c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5c887b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos = tweets_results.loc[:, 'tag']  # selección de las columnas de atributos\n",
    "objetivo = tweets_results['sentiment']  # selección de la columna objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3d2ca8e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 0.         -0.5         0.          0.          0.          1.\n  0.3         0.16        0.          0.1         0.5         0.5\n  0.          0.          0.          0.          0.5        -0.48214286\n  0.          0.          0.          0.06818182  0.18333333  0.\n -0.3125      0.35        0.          0.          0.3         0.\n  0.13636364  0.          0.2         0.5         0.3         0.\n -0.05681818  0.13636364  0.          0.46666667  0.          0.375\n  0.         -0.2         0.5         0.1        -0.29166667  0.\n -0.2         0.06818182 -0.8         0.          0.         -0.25\n  0.          0.         -0.3         0.          0.          0.3\n -0.2         0.2         0.          0.275       0.          0.\n  0.7         0.28571429  0.          0.          0.          0.56666667\n  0.          0.          0.7         0.25        0.1         0.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m codificador_atributos \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mOrdinalEncoder() \u001b[38;5;66;03m# --> transforma de discreta a ordinal\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcodificador_atributos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43matributos\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# --> entrena el modelo\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Categorías detectadas por el codificador para cada atributo\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(codificador_atributos\u001b[38;5;241m.\u001b[39mcategories_)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1258\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown_value should only be set when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle_unknown is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_encoded_value\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munknown_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;66;03m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[39;00m\n\u001b[1;32m-> 1258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_encoded_value\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_cats \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:74\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[1;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m X_list, n_samples, n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m n_features\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:46\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[1;34m(self, X, force_all_finite)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03mPerform custom check_array:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m- convert list of strings to object dtype\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# if not a dataframe, do normal check_array validation\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     X_temp \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(X_temp\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mstr_):\n\u001b[0;32m     48\u001b[0m         X \u001b[38;5;241m=\u001b[39m check_array(X, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    907\u001b[0m         )\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 0.         -0.5         0.          0.          0.          1.\n  0.3         0.16        0.          0.1         0.5         0.5\n  0.          0.          0.          0.          0.5        -0.48214286\n  0.          0.          0.          0.06818182  0.18333333  0.\n -0.3125      0.35        0.          0.          0.3         0.\n  0.13636364  0.          0.2         0.5         0.3         0.\n -0.05681818  0.13636364  0.          0.46666667  0.          0.375\n  0.         -0.2         0.5         0.1        -0.29166667  0.\n -0.2         0.06818182 -0.8         0.          0.         -0.25\n  0.          0.         -0.3         0.          0.          0.3\n -0.2         0.2         0.          0.275       0.          0.\n  0.7         0.28571429  0.          0.          0.          0.56666667\n  0.          0.          0.7         0.25        0.1         0.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "codificador_atributos = preprocessing.OrdinalEncoder() # --> transforma de discreta a ordinal\n",
    "codificador_atributos.fit(atributos) # --> entrena el modelo\n",
    "# Categorías detectadas por el codificador para cada atributo\n",
    "print(codificador_atributos.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f879ea4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fa0d9161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f99a95b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dea35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
