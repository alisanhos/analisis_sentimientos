{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae511335",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia Artificial\n",
    "\n",
    "## Análisis de sentimientos\n",
    "\n",
    "Realizado por:\n",
    "- Alicia Sánchez Hossdorf\n",
    "- Rafael Segura Gómez\n",
    "\n",
    "Fecha: 26/05/2023\n",
    "\n",
    "Convocatoria de Junio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2343c24",
   "metadata": {},
   "source": [
    "<h1> 1. Recopilación de datos. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0958c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba71c79",
   "metadata": {},
   "source": [
    "Hemos descargado todos los tweets que necesitamos en un CSV. Por lo que debemos leer los datos del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc99f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874309a0",
   "metadata": {},
   "source": [
    "Para saber cuantos tweets tenemos, miraremos cuantas filas tiene el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b203ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweetsCSV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3ada5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                I`d have responded, if I were going   neutral  \n",
       "1                                           Sooo SAD  negative  \n",
       "2                                        bullying me  negative  \n",
       "3                                     leave me alone  negative  \n",
       "4                                      Sons of ****,  negative  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                fun  positive  \n",
       "7                                         Soooo high   neutral  \n",
       "8                                        Both of you   neutral  \n",
       "9                       Wow... u just became cooler.  positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweetsCSV.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eee3ec",
   "metadata": {},
   "source": [
    "Como solo necesitamos la columna text, que es donde viene los tweets que necesitamos, haremos una seleccion de dicha columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d919204",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweetsCSV[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "670f9a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test from the LG enV2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S`ok, trying to plot alternatives as we speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i`ve been sick for the past few days  and thus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hes just not that into you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0                 I`d have responded, if I were going\n",
       "1       Sooo SAD I will miss you here in San Diego!!!\n",
       "2                           my boss is bullying me...\n",
       "3                      what interview! leave me alone\n",
       "4    Sons of ****, why couldn`t they put them on t...\n",
       "5   http://www.dothebouncy.com/smf - some shameles...\n",
       "6   2am feedings for the baby are fun when he is a...\n",
       "7                                          Soooo high\n",
       "8                                         Both of you\n",
       "9    Journey!? Wow... u just became cooler.  hehe....\n",
       "10   as much as i love to be hopeful, i reckon the...\n",
       "11  I really really like the song Love Story by Ta...\n",
       "12       My Sharpie is running DANGERously low on ink\n",
       "13  i want to go to music tonight but i lost my vo...\n",
       "14                         test test from the LG enV2\n",
       "15                              Uh oh, I am sunburned\n",
       "16   S`ok, trying to plot alternatives as we speak...\n",
       "17  i`ve been sick for the past few days  and thus...\n",
       "18         is back home now      gonna miss every one\n",
       "19                         Hes just not that into you"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd330f5a",
   "metadata": {},
   "source": [
    "<h1> 2. Eliminar las palabras que no aportan información </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7cc2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6bcf0bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003ce35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('punkt', download_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b52f1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'didn', 'but', 'do', 'i', 'him', 'too', 'has', 'in', 'yourselves', 'what', 're', \"mightn't\", 'against', 'd', 'most', \"mustn't\", \"she's\", 'yours', 'while', \"wouldn't\", 'wasn', 'doesn', \"won't\", 'can', 'just', 'from', 'off', 'further', 'when', 'your', 'isn', 'itself', 'more', 'herself', 'having', 'should', \"wasn't\", 'each', 'then', 'my', 'few', 'be', 'if', 'yourself', 'the', \"you've\", 'his', 'on', 'shan', 'which', \"hasn't\", 'of', 'so', 'does', 'because', 'have', 'being', 'other', 'don', \"needn't\", 'ma', \"it's\", \"you'll\", 'she', 'now', 'ourselves', 'both', 'them', 'wouldn', 'needn', \"weren't\", 'once', 'such', 'hasn', 'he', \"don't\", \"aren't\", \"doesn't\", 'her', 'after', 'where', 'below', 'that', 's', 'doing', 'how', 'into', 'those', 'no', \"you'd\", \"should've\", 'y', \"haven't\", 'up', 'not', 'it', 'won', 'did', 'we', 'me', 'had', 'down', 'been', 'was', 'll', 've', 't', 'our', \"that'll\", 'haven', 'an', 'out', 'mustn', 'a', 'o', \"couldn't\", 'by', 'with', 'their', 'as', 'myself', 'they', 'is', 'this', 'there', 'again', 'theirs', 'weren', 'than', 'm', 'are', 'very', 'at', 'or', 'until', 'himself', 'above', \"shan't\", 'why', 'these', \"hadn't\", 'shouldn', 'ain', 'over', 'whom', 'only', 'were', \"isn't\", 'before', 'for', 'here', 'all', 'under', 'nor', 'am', 'themselves', 'hers', 'mightn', 'and', 'between', 'any', 'same', 'some', 'hadn', 'through', 'couldn', \"you're\", 'own', 'during', 'about', \"didn't\", 'its', 'ours', 'who', 'to', 'aren', \"shouldn't\", 'you', 'will'}\n"
     ]
    }
   ],
   "source": [
    "sWords = set(stopwords.words('english'))\n",
    "print(sWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698cfa1",
   "metadata": {},
   "source": [
    "Crear una función clean_text para eliminar las palabras comunes y poco informativas; eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño; y que utilice técnicas de lematización (stemming) y tokenización para reducir las palabras a su forma base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a7bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar las stop words y tokenizado.\n",
    "\n",
    "def delete_stopWords(tweet):\n",
    "    tweetLow = tweet.lower()\n",
    "    tweetWords = word_tokenize(tweetLow)\n",
    "    tweetFiltered = []\n",
    "        \n",
    "    for w in tweetWords:\n",
    "        if w not in sWords:\n",
    "            tweetFiltered.append(w)\n",
    "    return tweetFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "301d7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion es para eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño.\n",
    "\n",
    "def delete_otherElements(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"http[^\\s]+\",\"\",tweet)\n",
    "    tweetFiltered = re.sub(r\"[-()\\\"#/@;:<>{}+=*~|.!?,]\", \"\", tweet)\n",
    "\n",
    "    return tweetFiltered\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb9f7",
   "metadata": {},
   "source": [
    "## Función para limpiar el text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe91be",
   "metadata": {},
   "source": [
    "Con los tweets tokenizados, lematizados y sin stop words lo pasamos a lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ea662b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(Tweets): \n",
    "    res = Tweets.copy()\n",
    "    i = 0;\n",
    "    #Iniciar SnowBall Stemmer con idioma ingles\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    while i<len(tweets):\n",
    "        tweet_cleaned = []\n",
    "        \n",
    "        clean_t = delete_otherElements(Tweets['text'].get(i))\n",
    "        \n",
    "        #__Para corregir gramaticalmente descomentar la siguiente linea__\n",
    "        #clean_t = str(TextBlob(clean_t).correct())\n",
    "        \n",
    "        tNoStopWords = delete_stopWords(clean_t)\n",
    "        #Esto es para la lematización de la frase. \n",
    "        for w in tNoStopWords:\n",
    "            if w != \"`\":\n",
    "                tweet_cleaned.append(stemmer.stem(w))\n",
    "        res.at[i,'text'] = tweet_cleaned\n",
    "        \n",
    "        i+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75694d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned = clean_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6ef69c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text\n",
      "0                                       [respond, go]\n",
      "1                       [sooo, sad, miss, san, diego]\n",
      "2                                       [boss, bulli]\n",
      "3                             [interview, leav, alon]\n",
      "4                 [son, put, releas, alreadi, bought]\n",
      "5       [shameless, plug, best, ranger, forum, earth]\n",
      "6                  [2am, feed, babi, fun, smile, coo]\n",
      "7                                       [soooo, high]\n",
      "8                                                  []\n",
      "9     [journey, wow, u, becam, cooler, hehe, possibl]\n",
      "10  [much, love, hope, reckon, chanc, minim, p, ne...\n",
      "11  [realli, realli, like, song, love, stori, tayl...\n",
      "12                    [sharpi, run, danger, low, ink]\n",
      "13             [want, go, music, tonight, lost, voic]\n",
      "14                             [test, test, lg, env2]\n",
      "15                                  [uh, oh, sunburn]\n",
      "16               [ok, tri, plot, altern, speak, sigh]\n",
      "17  [sick, past, day, thus, hair, look, wierd, did...\n",
      "18            [back, home, gon, na, miss, everi, one]\n",
      "19                                              [hes]\n"
     ]
    }
   ],
   "source": [
    "print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa48272",
   "metadata": {},
   "source": [
    "### A partir de aquí, hasta el etiquetado de datos, solamente es prueba de corrección gramatical, se puede ignorar, son pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5c7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrige el dataframe y muestra ambas columnas, corregido y original, no modifica variables originales\n",
    "#tweets_cleaned['text_corregido'] = tweets_cleaned['text'].apply(lambda x: str(TextBlob(\" \".join(x)).correct()))\n",
    "\n",
    "\n",
    "#print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1710b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra el dataframe con los textos corregidos\n",
    "#tweets_cleaned[['text_corregido']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1b3d",
   "metadata": {},
   "source": [
    "## Etiquetado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00e10b",
   "metadata": {},
   "source": [
    "Documentar:\n",
    "- !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1acaa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01e1847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_text(text):\n",
    "    tweet = ' '.join(text)  # Convertir la lista de palabras en una cadena de text\n",
    "    blob = TextBlob(tweet) #Convierte los tweets a tipo Blob para aplicarle la libreria\n",
    "    tag = blob.sentiment.polarity #Polaridad entre -1 y 1, de peor a mejor respectivamente\n",
    "    return text, tag\n",
    "\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "\n",
    "#Escritura en un archivo CSV nuevo\n",
    "with open(csv_path, 'w', newline='',encoding='utf-8-sig') as csv_file: #Forzamos formato utf-8, otro da problemas\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['text', 'tag', 'sentiment']) #Crea columna sentiment también\n",
    "    \n",
    "    if len(tweets_cleaned) == 0:  #Checkea si está vacío\n",
    "        print(\"Nothing to tag, espabila notas.\")\n",
    "        exit()\n",
    "\n",
    "    for _, row in tweets_cleaned.iterrows(): #Usamos iterrows porque tweets_cleaned es de tipo dataframe\n",
    "       # if row['text']:  # Verificar si el campo 'text' no está vacío\n",
    "            result = tag_text(row['text'])\n",
    "            result = (*result, '')  # Añadir una cadena vacía para la columna \"sentiment\"\n",
    "            writer.writerow(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ed685d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorizando los tags a valores discretos\n",
    "def etiqueta_codificada(etiqueta):\n",
    "    if etiqueta >= -1 and etiqueta < -0.5:\n",
    "        return \"hater\"\n",
    "    elif etiqueta >= -0.5 and etiqueta < 0:\n",
    "        return \"molesto\"\n",
    "    elif etiqueta == 0:\n",
    "        return \"neutro\"\n",
    "    elif etiqueta > 0 and etiqueta <= 0.5:\n",
    "        return \"contento\"\n",
    "    elif etiqueta > 0.5 and etiqueta <= 1:\n",
    "        return \"muy feliz\"\n",
    "    else:\n",
    "        return \"null\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4829e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actualizar la columna sentiment del csv\n",
    "# Leer el archivo CSV\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "df = pandas.read_csv(csv_path)\n",
    "\n",
    "# Aplicar la función etiqueta_codificada para asignar los valores de tag a sentiment\n",
    "df['sentiment'] = df['tag'].apply(etiqueta_codificada)\n",
    "\n",
    "\n",
    "\n",
    "# Guardar el archivo CSV actualizado\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98810428",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_results = pandas.read_csv(\"../Notebook/Tweets/resultados.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d2fc0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ea702d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['respond', 'go']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['sooo', 'sad', 'miss', 'san', 'diego']</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['boss', 'bulli']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['interview', 'leav', 'alon']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['son', 'put', 'releas', 'alreadi', 'bought']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['shameless', 'plug', 'best', 'ranger', 'forum...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>muy feliz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['2am', 'feed', 'babi', 'fun', 'smile', 'coo']</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['soooo', 'high']</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['journey', 'wow', 'u', 'becam', 'cooler', 'he...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>['much', 'love', 'hope', 'reckon', 'chanc', 'm...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>['realli', 'realli', 'like', 'song', 'love', '...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>['sharpi', 'run', 'danger', 'low', 'ink']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>['want', 'go', 'music', 'tonight', 'lost', 'vo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>['test', 'test', 'lg', 'env2']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>['uh', 'oh', 'sunburn']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>['ok', 'tri', 'plot', 'altern', 'speak', 'sigh']</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>['sick', 'past', 'day', 'thus', 'hair', 'look'...</td>\n",
       "      <td>-0.482143</td>\n",
       "      <td>molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>['back', 'home', 'gon', 'na', 'miss', 'everi',...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>['hes']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text       tag  sentiment\n",
       "0                                   ['respond', 'go']  0.000000     neutro\n",
       "1             ['sooo', 'sad', 'miss', 'san', 'diego'] -0.500000    molesto\n",
       "2                                   ['boss', 'bulli']  0.000000     neutro\n",
       "3                       ['interview', 'leav', 'alon']  0.000000     neutro\n",
       "4       ['son', 'put', 'releas', 'alreadi', 'bought']  0.000000     neutro\n",
       "5   ['shameless', 'plug', 'best', 'ranger', 'forum...  1.000000  muy feliz\n",
       "6      ['2am', 'feed', 'babi', 'fun', 'smile', 'coo']  0.300000   contento\n",
       "7                                   ['soooo', 'high']  0.160000   contento\n",
       "8                                                  []  0.000000     neutro\n",
       "9   ['journey', 'wow', 'u', 'becam', 'cooler', 'he...  0.100000   contento\n",
       "10  ['much', 'love', 'hope', 'reckon', 'chanc', 'm...  0.500000   contento\n",
       "11  ['realli', 'realli', 'like', 'song', 'love', '...  0.500000   contento\n",
       "12          ['sharpi', 'run', 'danger', 'low', 'ink']  0.000000     neutro\n",
       "13  ['want', 'go', 'music', 'tonight', 'lost', 'vo...  0.000000     neutro\n",
       "14                     ['test', 'test', 'lg', 'env2']  0.000000     neutro\n",
       "15                            ['uh', 'oh', 'sunburn']  0.000000     neutro\n",
       "16   ['ok', 'tri', 'plot', 'altern', 'speak', 'sigh']  0.500000   contento\n",
       "17  ['sick', 'past', 'day', 'thus', 'hair', 'look'... -0.482143    molesto\n",
       "18  ['back', 'home', 'gon', 'na', 'miss', 'everi',...  0.000000     neutro\n",
       "19                                            ['hes']  0.000000     neutro"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 primeras filas\n",
    "tweets_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b249095",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "Necesitamos separar los datos que tenemos en conjunto de entrenamiento y conjunto de pruebas, para probar que funciona bien e ir entrenando el modelo.\n",
    "\n",
    "A partir de aqui, pruebitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4b26a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bd4b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cojo la columna text\n",
    "tweets = tweets_results['text']\n",
    "\n",
    "#Creamos el vector\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#Vectorizamos los tweets\n",
    "codified_text = vect.fit_transform(tweets)\n",
    "\n",
    "#Averlo?\n",
    "#print(codified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4e86ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 4 ... 3 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Codificador para el objetivo\n",
    "cod_target = preprocessing.LabelEncoder()\n",
    "\n",
    "#Sacamos la columna sentiment del CSV de results\n",
    "codified_target = cod_target.fit_transform(tweets_results['sentiment'])\n",
    "\n",
    "#Printi printi\n",
    "print(codified_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3414af9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  (18411, 22865)\n",
      "Test data size:  (9069, 22865)\n"
     ]
    }
   ],
   "source": [
    "train_text, test_text, train_target, test_target = train_test_split(codified_text, codified_target, \n",
    "                                                                    random_state=12345, \n",
    "                                                                    test_size=0.33)\n",
    "\n",
    "print('Training data size: ', train_text.shape)\n",
    "print('Test data size: ', test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19594851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tfidf data size:  (18411, 22865)\n",
      "Test tfidf data size:  (9069, 22865)\n"
     ]
    }
   ],
   "source": [
    "#Tf-idf\n",
    "\n",
    "#Creamos instancia vector tfidf\n",
    "vectorizer_tfidf = TfidfTransformer()\n",
    "\n",
    "train_text_tfidf = vectorizer_tfidf.fit_transform(train_text)\n",
    "\n",
    "test_text_tfidf = vectorizer_tfidf.transform(test_text)\n",
    "\n",
    "print('Training tfidf data size: ', train_text_tfidf.shape)\n",
    "print('Test tfidf data size: ', test_text_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e507b2",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Con los conjuntos de prueba y entrenamiento en tf-idf vamos a entrenar los modelos, empezamos por Naive Bayes. Usamos MultinomialNB porque tenemos un vector disperso de datos, funcionando bien para conjunto de datos enormes (como nuestro caso), para trabajar con CategoricalNB haría falta crear una matriz densa y eso puede dar problemas si el conjunto de datos es enorme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8f07221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5a7f4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos el modelo, con suavizado (alpha=1.0)\n",
    "modelNB = MultinomialNB(alpha=1.0)\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelNB.fit(train_text, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "736fcc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto, calculada con predict           0.7706472598963502\n",
      "Tasa de acierto, calculada con accuracy_score    0.7706472598963502\n"
     ]
    }
   ],
   "source": [
    "#Hacemos prediccion con los datos de prueba\n",
    "testNB = modelNB.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelNB.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48839da8",
   "metadata": {},
   "source": [
    "### k-Nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6328e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b857a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelKNN = KNeighborsClassifier(n_neighbors=5, metric='cosine') #Usamos coseno porque es mejor para clasificar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1428485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(metric=&#x27;cosine&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(metric='cosine')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos el modelo con los parametros deseados\n",
    "modelKNN = KNeighborsClassifier(n_neighbors=5, metric='cosine') #Usamos coseno porque es mejor para clasificar texto\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelKNN.fit(train_text, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2ca6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto, calculada con predict           0.6572940787297387\n",
      "Tasa de acierto, calculada con accuracy_score    0.6572940787297387\n"
     ]
    }
   ],
   "source": [
    "#Hacemos prediccion con los datos de prueba\n",
    "testKNN = modelKNN.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelKNN.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testKNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904323c4",
   "metadata": {},
   "source": [
    "### Arbolito arbolito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7112ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7ea6a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=3, random_state=12345)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=3, random_state=12345)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=12345)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos modelo con arbol, profundidad 3, criterio Entropy\n",
    "modelTree = DecisionTreeClassifier(max_depth=3, criterion='entropy', random_state=12345)\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelTree.fit(train_text, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50985327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto, calculada con predict           0.5693020178630499\n",
      "Tasa de acierto, calculada con accuracy_score    0.5693020178630499\n"
     ]
    }
   ],
   "source": [
    "#Hacemos prediccion con los datos de prueba\n",
    "testTree = modelTree.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelTree.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9144cb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto, calculada con predict           0.608556621457713\n",
      "Tasa de acierto, calculada con accuracy_score    0.608556621457713\n"
     ]
    }
   ],
   "source": [
    "#Creamos modelo con arbol, profunidad 5, criterio Gini\n",
    "modelTree2 = DecisionTreeClassifier(max_depth=5, criterion='gini', random_state=12345)\n",
    ",\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelTree2.fit(train_text, train_target)\n",
    "\n",
    "#Hacemos prediccion con los datos de prueba\n",
    "testTree2 = modelTree2.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelTree2.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testTree2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbd329",
   "metadata": {},
   "source": [
    "Comprobamos que el árbol de decisión con criterio de entropía y profundidad 3 conseguimos la tasa de aciertos más alta (hemos probado valores).\n",
    "\n",
    "\n",
    "Invertimos los valores de los objetivos para comprobar los datos y ver como ha predecido (spoiler: mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f35b3d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutro', 'muy feliz', 'neutro', ..., 'contento', 'contento',\n",
       "       'neutro'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentimientos antes del modelo\n",
    "\n",
    "cod_target.inverse_transform(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92704ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutro', 'contento', 'neutro', ..., 'contento', 'contento',\n",
       "       'neutro'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentimientos despues del modelo\n",
    "\n",
    "cod_target.inverse_transform(testNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee24d6",
   "metadata": {},
   "source": [
    "## Analisis de tweets\n",
    "Hacemos scrapping de algun usuario de Twitter, algun influencer, y analizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d48bca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium is already installed\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "if importlib.util.find_spec(\"selenium\") is None:\n",
    "    !pip install selenium\n",
    "    print(\"Installed selenium succesfully\")\n",
    "else:\n",
    "    print(\"Selenium is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7500d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los nuevos tweets se han guardado en C:\\Users\\rafse\\Documents\\Proyecto IA\\Notebook\\influencers\\tweets_Pontifex.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_tweets(username, amount=10):\n",
    "    all_the_tweets = []  # Una lista para almacenar todos los tweets de una página\n",
    "    driver = webdriver.Chrome()  # Requiere tener instalado el driver de Chrome\n",
    "    driver.get(f\"https://twitter.com/{username}\")\n",
    "    sleep_time = 3\n",
    "    time.sleep(sleep_time)  # Espera unos segundos para que se carguen los tweets\n",
    "\n",
    "    # Repetir el proceso hasta que tenga 100 tweets\n",
    "    while len(all_the_tweets) < amount:\n",
    "        # Desplazarse hacia abajo para cargar más tweets (opcional)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_time)  # Espera adicional para cargar más tweets si es necesario\n",
    "\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        tweets = soup.find_all(\"div\", attrs={\"data-testid\": \"tweetText\"})\n",
    "        all_the_tweets += tweets\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    if all_the_tweets:\n",
    "        # Obtener la ruta relativa al directorio del proyecto\n",
    "        script_directory = os.path.join(os.getcwd(), \"influencers\")\n",
    "        # crear el directorio si no existe\n",
    "        if not os.path.exists(script_directory):\n",
    "            os.makedirs(script_directory)\n",
    "            print(f\"Se ha creado el directorio {script_directory}\")\n",
    "        file_path = os.path.join(script_directory, f\"tweets_{username}.csv\")\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\", newline=\"\",) as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"text\"])  # Escribir la fila de encabezado\n",
    "            for tweet in all_the_tweets:\n",
    "                tweet_span = tweet.find_next(\"span\")\n",
    "                tweet_text = tweet_span.get_text(strip=True)\n",
    "                tweet_text = tweet_text.replace(\"\\n\", \" \")  # Eliminar los saltos de línea del contenido del tweet\n",
    "                writer.writerow([tweet_text])\n",
    "\n",
    "              #  print(tweet_text)\n",
    "              #  print(\"------------------------\")\n",
    "\n",
    "        print(f\"Los nuevos tweets se han guardado en {file_path}\")\n",
    "    else:\n",
    "        print(\"No se encontró el contenedor de tweets.\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "scrape_tweets(\"elonmusk\", 100)\n",
    "scrape_tweets(\"LinusTech\", 100)\n",
    "scrape_tweets(\"Pontifex\", 100)\n",
    "scrape_tweets(\"realDonaldTrump\", 100)\n",
    "scrape_tweets(\"Cobratate\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2161762",
   "metadata": {},
   "source": [
    "Analizamos los tuits de los influencers, para ver como de capullos son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7ce59c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1)\n",
      "                                                text\n",
      "0  We should not be afraid of proclaiming the tru...\n",
      "1  We are the dust of the earth, upon which God h...\n",
      "2  Christians do not diminish the seriousness of ...\n",
      "3  Jesus ascends to the Father to intercede on ou...\n",
      "4  Amidst the hardships and difficulties of the m...\n",
      "5                                           Building\n",
      "6                                                The\n",
      "7  The Holy Spirit is demanding, because He is a ...\n",
      "8                                                May\n",
      "9                                             #Birth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pontifex_tweetsCSV = pandas.read_csv(\"../Notebook/influencers/tweets_Pontifex.csv\", header=0)\n",
    "print(pontifex_tweetsCSV.shape)\n",
    "print(pontifex_tweetsCSV.head(10))\n",
    "pontifex_tweets_df = pontifex_tweetsCSV[['text']]\n",
    "type(pontifex_tweets_df)\n",
    "\n",
    "pontifex_tweets = pontifex_tweets_df.squeeze()\n",
    "\n",
    "type(pontifex_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9f1ba5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pontifex_series = pontifex_tweets_df['text']\n",
    "pontifex_series\n",
    "\n",
    "type(pontifex_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "da010c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We should not be afraid of proclaiming the tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We are the dust of the earth, upon which God h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christians do not diminish the seriousness of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jesus ascends to the Father to intercede on ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amidst the hardships and difficulties of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Holy Spirit is demanding, because He is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Birth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  We should not be afraid of proclaiming the tru...\n",
       "1  We are the dust of the earth, upon which God h...\n",
       "2  Christians do not diminish the seriousness of ...\n",
       "3  Jesus ascends to the Father to intercede on ou...\n",
       "4  Amidst the hardships and difficulties of the m...\n",
       "5                                           Building\n",
       "6                                                The\n",
       "7  The Holy Spirit is demanding, because He is a ...\n",
       "8                                                May\n",
       "9                                             #Birth"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pontifex_tweetsCSV.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4d097dc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Too many indexers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tweets_cleaned_pontifex \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpontifex_tweets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tweets_cleaned_pontifex\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m))\n",
      "Cell \u001b[1;32mIn[106], line 8\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(Tweets)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(Tweets):\n\u001b[0;32m      7\u001b[0m     tweet_cleaned \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m     clean_t \u001b[38;5;241m=\u001b[39m delete_otherElements(\u001b[43mTweets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# __Para corregir gramaticalmente descomentar la siguiente linea__\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# clean_t = str(TextBlob(clean_t).correct())\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     tNoStopWords \u001b[38;5;241m=\u001b[39m delete_stopWords(clean_t)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1066\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1250\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_lowerdim(tup)\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m-> 1250\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_tuple_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;66;03m# ugly hack for GH #836\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:869\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_tuple_indexer\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    Check the key for valid keys across my indexer.\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_key_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(key)\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:908\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_key_length\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(_one_ellipsis_message)\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key_length(key)\n\u001b[1;32m--> 908\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many indexers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m key\n",
      "\u001b[1;31mIndexingError\u001b[0m: Too many indexers"
     ]
    }
   ],
   "source": [
    "tweets_cleaned_pontifex = clean_text(pontifex_tweets)\n",
    "\n",
    "print(tweets_cleaned_pontifex.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f0036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9367ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
