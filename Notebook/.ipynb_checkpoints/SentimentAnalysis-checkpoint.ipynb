{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae511335",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia Artificial\n",
    "\n",
    "## Análisis de sentimientos\n",
    "\n",
    "Realizado por:\n",
    "- Alicia Sánchez Hossdorf\n",
    "- Rafael Segura Gómez\n",
    "\n",
    "Fecha: 26/05/2023\n",
    "\n",
    "Convocatoria de Junio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2343c24",
   "metadata": {},
   "source": [
    "<h1> 1. Recopilación de datos. </h1>\n",
    "\n",
    "Importamos un par de bibliotecas que nos haran falta posteriormente, y comprobación de si tenemos ya instaladas las librerias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if importlib.util.find_spec(\"selenium\") is None:\n",
    "    !pip install selenium\n",
    "    print(\"Installed selenium succesfully\")\n",
    "else:\n",
    "    print(\"Selenium is already installed\")\n",
    "    \n",
    "if importlib.util.find_spec(\"textblob\") is None:\n",
    "    !pip install textblob\n",
    "    print(\"Installed Textblob succesfully\")\n",
    "else:\n",
    "    print(\"Textblob is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba71c79",
   "metadata": {},
   "source": [
    "Hemos descargado todos los tweets que necesitamos en un CSV. Por lo que debemos leer los datos del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsCSV = pandas.read_csv(\"../Notebook/Tweets/Tweets_entrenamiento.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874309a0",
   "metadata": {},
   "source": [
    "Para saber cuantos tweets tenemos, miraremos cuantas filas tiene el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b203ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweetsCSV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ada5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 primeras filas\n",
    "tweetsCSV.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eee3ec",
   "metadata": {},
   "source": [
    "Como solo necesitamos la columna text, que es donde estan los tweets que necesitamos, haremos una seleccion de dicha columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d919204",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweetsCSV[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd330f5a",
   "metadata": {},
   "source": [
    "<h1> 2. Eliminar las palabras que no aportan información </h1>\n",
    "\n",
    "Importamos las bibliotecas de NLTK y Regex, para filtrar, tokenizar y, en general, tratar el texto para simplificarlo y poder trabajarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcf0bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ce35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('punkt', download_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "sWords = set(stopwords.words('english'))\n",
    "print(sWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698cfa1",
   "metadata": {},
   "source": [
    "Crear una función clean_text para eliminar las palabras comunes y poco informativas; eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño; y que utilice técnicas de lematización (stemming) y tokenización para reducir las palabras a su forma base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar las stop words y tokenizado.\n",
    "\n",
    "def delete_stopWords(tweet):\n",
    "    tweetLow = tweet.lower()\n",
    "    tweetWords = word_tokenize(tweetLow)\n",
    "    tweetFiltered = []\n",
    "        \n",
    "    for w in tweetWords:\n",
    "        if w not in sWords:\n",
    "            tweetFiltered.append(w)\n",
    "    return tweetFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion es para eliminar menciones, hashtags, URLs y cualquier otro símbolo extraño mediante regex.\n",
    "\n",
    "def delete_otherElements(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    tweet = re.sub(\"http[^\\s]+\",\"\",tweet)\n",
    "    tweetFiltered = re.sub(r\"[-()\\\"#/@;:<>{}+=*~|.!?,]\", \"\", tweet)\n",
    "\n",
    "    return tweetFiltered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb9f7",
   "metadata": {},
   "source": [
    "## Función para limpiar el text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe91be",
   "metadata": {},
   "source": [
    "Con los tweets tokenizados, lematizados y sin stop words lo pasamos a lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea662b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(Tweets): \n",
    "    res = Tweets.copy()\n",
    "    i = 0;\n",
    "    #Iniciar SnowBall Stemmer con idioma ingles\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    while i<len(Tweets):\n",
    "        tweet_cleaned = []\n",
    "        \n",
    "        clean_t = delete_otherElements(Tweets['text'].get(i))\n",
    "        \n",
    "        #__Para corregir gramaticalmente descomentar la siguiente linea__\n",
    "        #clean_t = str(TextBlob(clean_t).correct())\n",
    "        \n",
    "        tNoStopWords = delete_stopWords(clean_t)\n",
    "        #Esto es para la lematización de la frase. \n",
    "        for w in tNoStopWords:\n",
    "            if w != \"`\":\n",
    "                tweet_cleaned.append(stemmer.stem(w))\n",
    "        res.at[i,'text'] = tweet_cleaned\n",
    "        \n",
    "        i+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75694d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos los tweets aplicandole la función\n",
    "tweets_cleaned = clean_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa48272",
   "metadata": {},
   "source": [
    "Se implementó una correción gramatical de los tweets pero es inviable en un set de datos de semejante tamaño, lo comentamos para no usarlo ahora, pero lo dejamos por si hiciera falta la funcionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrige el dataframe y muestra ambas columnas, corregido y original, no modifica variables originales\n",
    "#tweets_cleaned['text_corregido'] = tweets_cleaned['text'].apply(lambda x: str(TextBlob(\" \".join(x)).correct()))\n",
    "\n",
    "\n",
    "#print(tweets_cleaned.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1710b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra el dataframe con los textos corregidos\n",
    "#tweets_cleaned[['text_corregido']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1b3d",
   "metadata": {},
   "source": [
    "## Etiquetado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00e10b",
   "metadata": {},
   "source": [
    "Creamos la funcion para etiquetar texto y ver la polaridad, el sentimiento, entre -1 y 1 de peor a mejor, respectivamente. Escribimos en un archivo _resultado.csv_ los textos con la columna de la polaridad correspondiente, para trabajarla posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text):\n",
    "    tweet = ' '.join(text)  # Convertir la lista de palabras en una cadena de text\n",
    "    blob = TextBlob(tweet) #Convierte los tweets a tipo Blob para aplicarle la libreria\n",
    "    tag = blob.sentiment.polarity #Polaridad entre -1 y 1, de peor a mejor respectivamente\n",
    "    return text, tag\n",
    "\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "\n",
    "#Escritura en un archivo CSV nuevo\n",
    "with open(csv_path, 'w', newline='',encoding='utf-8-sig') as csv_file: #Forzamos formato utf-8, otro da problemas\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['text', 'tag', 'sentiment']) #Crea columna sentiment también\n",
    "    \n",
    "    if len(tweets_cleaned) == 0:  #Checkea si está vacío\n",
    "        print(\"Nothing to tag, espabila notas.\")\n",
    "        exit()\n",
    "\n",
    "    for _, row in tweets_cleaned.iterrows(): #Usamos iterrows porque tweets_cleaned es de tipo dataframe\n",
    "       # if row['text']:  # Verificar si el campo 'text' no está vacío\n",
    "            result = tag_text(row['text'])\n",
    "            result = (*result, '')  # Añadir una cadena vacía para la columna \"sentiment\"\n",
    "            writer.writerow(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa76a0",
   "metadata": {},
   "source": [
    "Clasificamos los sentimientos entre -1 y 1. Se ha elegido 0 como neutro, de -1 a -0.5 para ser hater, de -0.5 a 0 para ser molesto y simétricamente para los positivos, de 0 a 0.5 para contento y de 0.5 a 1 para muy feliz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed685d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorizando los tags a valores discretos\n",
    "def etiqueta_codificada(etiqueta):\n",
    "    if etiqueta >= -1 and etiqueta < -0.5:\n",
    "        return \"hater\"\n",
    "    elif etiqueta >= -0.5 and etiqueta < 0:\n",
    "        return \"molesto\"\n",
    "    elif etiqueta == 0:\n",
    "        return \"neutro\"\n",
    "    elif etiqueta > 0 and etiqueta <= 0.5:\n",
    "        return \"contento\"\n",
    "    elif etiqueta > 0.5 and etiqueta <= 1:\n",
    "        return \"muy feliz\"\n",
    "    else:\n",
    "        return \"null\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c248d02",
   "metadata": {},
   "source": [
    "Actualizamos el _resultados.csv_ con la columna _Sentiment_, el valor discreto relacionado al ordinal del _tag_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizar la columna sentiment del csv\n",
    "csv_path = 'Tweets/resultados.csv'\n",
    "df = pandas.read_csv(csv_path)\n",
    "\n",
    "# Aplicar la función etiqueta_codificada para asignar los valores de tag a sentiment\n",
    "df['sentiment'] = df['tag'].apply(etiqueta_codificada)\n",
    "\n",
    "# Guardar el archivo CSV actualizado\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98810428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la variable con la lectura del archivo resultado.csv\n",
    "tweets_results = pandas.read_csv(\"../Notebook/Tweets/resultados.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fc0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos visualmente si está bien\n",
    "print(tweets_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea702d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 primeras filas\n",
    "tweets_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b249095",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "Necesitamos separar los datos que tenemos en conjunto de entrenamiento y conjunto de pruebas, para probar que funciona bien e ir entrenando el modelo.\n",
    "\n",
    "Importamos todas las librerias necesarias para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b26a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14288914",
   "metadata": {},
   "source": [
    "Sacamos la columna _tweets_ del CSV. Creamos el vectorizador vacío para codificar el texto en un vector disperso y poder trabajar facilmente con los datos, ya que son miles de palabras diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cojo la columna text\n",
    "tweets = tweets_results['text']\n",
    "\n",
    "#Creamos el vector\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#Vectorizamos los tweets\n",
    "codified_text = vect.fit_transform(tweets)\n",
    "\n",
    "#Averlo?\n",
    "#print(codified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772104b3",
   "metadata": {},
   "source": [
    "Codificamos el objetivo, preprocesandolo previamente. Necesitamos dar valores ordinarios a los valores discretos originales, podríamos trabajar sobre la polarización entre -1 y 1 original pero crearía valores muy dispersos y así los unificamos, no tenemos infinitos posibles valores si no solamente 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e86ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codificador para el objetivo\n",
    "cod_target = preprocessing.LabelEncoder()\n",
    "\n",
    "#Sacamos la columna sentiment del CSV de results\n",
    "codified_target = cod_target.fit_transform(tweets_results['sentiment'])\n",
    "\n",
    "#Printi printi\n",
    "print(codified_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89a1a7",
   "metadata": {},
   "source": [
    "Creamos los conjuntos de entrenamiento y de prueba cogiendo el 67% aprox para entrenamiento y el 30% para prueba y ver como de preciso es nuestro modelo. Pondremos una semilla concreta de _12345_ que mantendremos durante todo el código y así trabajar siemrpe con la misma randomización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, train_target, test_target = train_test_split(codified_text, codified_target, \n",
    "                                                                    random_state=12345, \n",
    "                                                                    test_size=0.33)\n",
    "\n",
    "print('Training data size: ', train_text.shape)\n",
    "print('Test data size: ', test_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422e55a",
   "metadata": {},
   "source": [
    "Aplicamos _TF-IDF (Term Frequency-Inverse Document Frequency)_ a los conjuntos para así darle la importancia que merecen a las palabras que más salgan, que más se repitan, a las menos comunes... Y así entrenar el modelo con pesos, minimizando la importancia de palabras poco frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19594851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-idf\n",
    "\n",
    "#Creamos instancia vector tfidf\n",
    "vectorizer_tfidf = TfidfTransformer()\n",
    "\n",
    "train_text_tfidf = vectorizer_tfidf.fit_transform(train_text)\n",
    "\n",
    "test_text_tfidf = vectorizer_tfidf.transform(test_text)\n",
    "\n",
    "print('Training tfidf data size: ', train_text_tfidf.shape)\n",
    "print('Test tfidf data size: ', test_text_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e507b2",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Con los conjuntos de prueba y entrenamiento en tf-idf vamos a entrenar los modelos, empezamos por Naive Bayes. Usamos MultinomialNB porque tenemos un vector disperso de datos, funcionando bien para conjunto de datos enormes (como nuestro caso), para trabajar con CategoricalNB haría falta crear una matriz densa y eso puede dar problemas si el conjunto de datos es enorme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f07221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el modelo, con suavizado (alpha=1.0)\n",
    "modelNB = MultinomialNB(alpha=1.0)\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelNB.fit(train_text, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos prediccion con los datos de prueba\n",
    "testNB = modelNB.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelNB.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48839da8",
   "metadata": {},
   "source": [
    "### k-Nn\n",
    "\n",
    "Al modelo kNn, k-n vecinos, elegiremos que n sea 5 por ser el estandar y el coseno porque es el mejor para comparar vectores que es, en definitiva, lo que tenemos, vectores dispersos con las _\"palabras\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b857a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelKNN = KNeighborsClassifier(n_neighbors=5, metric='cosine') #Usamos coseno porque es mejor para clasificar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1428485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el modelo con los parametros deseados\n",
    "modelKNN = KNeighborsClassifier(n_neighbors=5, metric='cosine') #Usamos coseno porque es mejor para clasificar texto\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelKNN.fit(train_text, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos prediccion con los datos de prueba\n",
    "testKNN = modelKNN.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelKNN.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testKNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904323c4",
   "metadata": {},
   "source": [
    "### Árboles de decisión.\n",
    "\n",
    "En este modelo hemos usado comparación cruzada, aunque solo mostramos dos de ellas por reducir el código innecesario. En una opción hemos usado el criterio de _Entropía_, con la profundidad del arbol _3_, y en otro hemos usado _Gini_ con profundidad _5_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos modelo con arbol, profundidad 3, criterio Entropy\n",
    "modelTree = DecisionTreeClassifier(max_depth=3, criterion='entropy', random_state=12345)\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelTree.fit(train_text, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50985327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos prediccion con los datos de prueba\n",
    "testTree = modelTree.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelTree.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f31976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos modelo con arbol, profunidad 5, criterio Gini\n",
    "modelTree2 = DecisionTreeClassifier(max_depth=5, criterion='gini', random_state=12345)\n",
    ",\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "modelTree2.fit(train_text, train_target)\n",
    "\n",
    "#Hacemos prediccion con los datos de prueba\n",
    "testTree2 = modelTree2.predict(test_text)\n",
    "\n",
    "print('Tasa de acierto, calculada con predict          ', modelTree2.score(test_text,test_target))\n",
    "print('Tasa de acierto, calculada con accuracy_score   ', accuracy_score(test_target, testTree2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbd329",
   "metadata": {},
   "source": [
    "Comprobamos que el árbol de decisión con criterio de Gini y profundidad 5 conseguimos la tasa de aciertos más alta, aunque no por demasiado.\n",
    "\n",
    "\n",
    "Invertimos los valores de los objetivos para comprobar los datos y ver como ha predecido.\n",
    "\n",
    "#### Comprobación de valores\n",
    "Vemos que el modelo con mejor tasa de aciertos es Naive Bayes, lo usaremos más adelante como el modelo elegido.\n",
    "Hacemos la inversa de los objetivos antes y después del modelo para comprobar cómo de acertado ha estado, aunque vemos que no demasiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentimientos antes del modelo\n",
    "\n",
    "cod_target.inverse_transform(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92704ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentimientos despues del modelo\n",
    "\n",
    "cod_target.inverse_transform(testNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c46e5",
   "metadata": {},
   "source": [
    "## Analisis de tweets\n",
    "Hacemos scrapping de algun usuario de Twitter, algun influencer, y analizamos tras aplicar el modelo entrenado. Hemos creado un script para hacer scrapping directamente de la página de Twitter, con _selenium_ y _BeautifulSoup_. El script abre el perfil de Twitter del usuario indicado con una instancia de Chromium automatizada con _selenium_, espera 3 segundos y scrollea hasta el final de la página, cargando todos los tweets que va guardando temporalmente. Realiza ese mismo procedimiento las veces que sea necesario hasta que llegue al mínimo indicado, y los guarda en un archivo _csv_ personalizado. \n",
    "\n",
    "La lista de usuarios de los que obtenemos tweets y el numero de tweets que conseguimos se edita al fondo del script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_tweets(username, amount=10):\n",
    "    all_the_tweets = []  # Una lista para almacenar todos los tweets de una página\n",
    "    driver = webdriver.Chrome()  # Requiere tener instalado el driver de Chrome\n",
    "    driver.get(f\"https://twitter.com/{username}\")\n",
    "    sleep_time = 3\n",
    "    time.sleep(sleep_time)  # Espera unos segundos para que se carguen los tweets\n",
    "\n",
    "    # Repetir el proceso hasta que tenga 100 tweets\n",
    "    while len(all_the_tweets) < amount:\n",
    "        # Desplazarse hacia abajo para cargar más tweets (opcional)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_time)  # Espera adicional para cargar más tweets si es necesario\n",
    "\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        tweets = soup.find_all(\"div\", attrs={\"data-testid\": \"tweetText\"})\n",
    "        all_the_tweets += tweets\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    if all_the_tweets:\n",
    "        # Obtener la ruta relativa al directorio del proyecto\n",
    "        script_directory = os.path.join(os.getcwd(), \"influencers\")\n",
    "        # crear el directorio si no existe\n",
    "        if not os.path.exists(script_directory):\n",
    "            os.makedirs(script_directory)\n",
    "            print(f\"Se ha creado el directorio {script_directory}\")\n",
    "        file_path = os.path.join(script_directory, f\"tweets_{username}.csv\")\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\", newline=\"\",) as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"text\"])  # Escribir la fila de encabezado\n",
    "            for tweet in all_the_tweets:\n",
    "                tweet_span = tweet.find_next(\"span\")\n",
    "                tweet_text = tweet_span.get_text(strip=True)\n",
    "                tweet_text = tweet_text.replace(\"\\n\", \" \")  # Eliminar los saltos de línea del contenido del tweet\n",
    "                writer.writerow([tweet_text])\n",
    "\n",
    "              #  print(tweet_text)\n",
    "              #  print(\"------------------------\")\n",
    "\n",
    "        print(f\"Los nuevos tweets se han guardado en {file_path}\")\n",
    "    else:\n",
    "        print(\"No se encontró el contenedor de tweets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2155f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de usuarios a extraer tweets\n",
    "usernames=['cobratate','xQc','elonmusk','jk_rowling','pontifex','bbcnews','AlertWorld_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11898e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecución del script\n",
    "for user in usernames:\n",
    "    scrape_tweets(user, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f3f0d",
   "metadata": {},
   "source": [
    "Creamos un script para limpiar los tweets de los usuarios tal cual hicimos con el data set inicial, entrenamos el vectorizador con nuestro modelo y codificamos el texto, y predecimos según el modelo (Naive Bayes en nuestro caso). Luego representamos en un _pie chart_ los sentimientos de cada usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sentiments(username: str):\n",
    "    try:\n",
    "        user_tweetsCSV = pandas.read_csv(f\"../Notebook/influencers/tweets_{username}.csv\", header=0)\n",
    "        print(f'Analizando username --> {username}')\n",
    "        #print(user_tweetsCSV.shape)\n",
    "        user_tweets = user_tweetsCSV[['text']]\n",
    "\n",
    "        user_cleaned = clean_text(user_tweets)\n",
    "\n",
    "        # Codificamos los tweets para la predicción \n",
    "\n",
    "        tweets_user = user_cleaned['text']\n",
    "\n",
    "        # Convertimos las listas de palabras en cadenas de texto\n",
    "        tweets_user_str = [' '.join(tweet) for tweet in tweets_user]\n",
    "\n",
    "        vect.fit(tweets)\n",
    "\n",
    "        # Utilizamos el vectorizador previamente ajustado para transformar los tweets \n",
    "        codified_text_user = vect.transform(tweets_user_str)\n",
    "\n",
    "        userNB = modelNB.predict(codified_text_user)\n",
    "\n",
    "        tag, freq = numpy.unique(cod_target.inverse_transform(userNB), return_counts=True)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.pie(freq, labels=tag, autopct='%1.1f%%')\n",
    "        plt.title(f'Sentimientos de {username}')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for username '{username}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9082a28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mostrar pie chart con los sentimientos\n",
    "for user in usernames:\n",
    "    sentiments(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977c30d",
   "metadata": {},
   "source": [
    "#### Conclusión\n",
    "Ninguno de los usuarios analizados, algunos conocidos por ser personas tóxicas, personas bastante hater en general pero que no trasladan sus pensamientos directamente a sus tweets, o no usan palabras clasificadas como malas y por eso la mayoría aparecen como personas contentas y, en su mayoria, neutrales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
